selenium
========

安装
----

[参考这个网页](https://github.com/appium/python-client/issues/863)，注意会有一个bug，要安装4.9.0版本的selenium

浏览器
------

要安装浏览器和对应版本的driver，chrome[浏览器下载](https://www.google.com/intl/en_sg/chrome/browser-tools/)，chrome driver[参考这个网页](https://googlechromelabs.github.io/chrome-for-testing/)

[将整个网页长截图](https://blog.csdn.net/yutu75/article/details/115524985)
--------------------------------------------------------------------------

```python
import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
def get_image(url, pic_name):
    """
    #设置chrome开启的模式，headless就是无界面模式
    # 创建一个参数对象，用来控制chrome以无界面模式打开
    :param url:             获取获取网页的地址
    :param pic_name:        需要保存的文件名或路径＋文件名
    :return:
    """
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--disable-gpu')
    # 创建浏览器对象
    driver = webdriver.Chrome(chrome_options=chrome_options)
    # 打开网页
    driver.get(url)
    # driver.maximize_window()
    # 加延时 防止未加载完就截图
    time.sleep(1)
    # 用js获取页面的宽高，如果有其他需要用js的部分也可以用这个方法
    width = driver.execute_script("return document.documentElement.scrollWidth")
    height = driver.execute_script("return document.documentElement.scrollHeight")
    # 获取页面宽度及其宽度
    print(width,height)
    # 将浏览器的宽高设置成刚刚获取的宽高
    driver.set_window_size(width, height)
    time.sleep(1)
    # 截图并关掉浏览器
    driver.get_screenshot_as_file(pic_name)
    driver.quit()
# 你输入的参数
url_str = 'http://www.cq.gov.cn'
pic_name = r'qwq.png'
get_image(url_str, pic_name)
```



[测试脚本](https://www.selenium.dev/zh-cn/documentation/webdriver/getting_started/first_script/)
--------

```python 
from selenium import webdriver
from selenium.webdriver.common.by import By
import time
driver = webdriver.Chrome()

driver.get("https://www.selenium.dev/selenium/web/web-form.html")

title = driver.title

driver.implicitly_wait(0.5)

text_box = driver.find_element(by=By.NAME, value="my-text")
submit_button = driver.find_element(by=By.CSS_SELECTOR, value="button")

text_box.send_keys("Selenium")
submit_button.click()

message = driver.find_element(by=By.ID, value="message")
text = message.text
time.sleep(5)
driver.quit()

```

启动浏览器的时候加载用户的配置
------------------------------

[我尝试了里面的方法，但是没有成功](https://blog.csdn.net/zhusongziye/article/details/79636567)。

[元素选择](https://juejin.cn/post/6844904002103050254)
------------------------------------------------------

当使用CSS选择器时，我们可以通过多种方法来精确地选择页面上的元素。以下是常见的选择器及其用法总结：

### 1. 基本选择器

- **标签选择器**：根据 HTML 元素的标签名选择元素。

  ```css
  div {}   /* 选择所有 <div> 元素 */
  p {}     /* 选择所有 <p> 元素 */
  ```

- **类选择器**：根据 HTML 元素的类名选择元素。

  ```css
  .class-name {}   /* 选择所有具有 class="class-name" 的元素 */
  ```

- **ID 选择器**：根据 HTML 元素的 ID 属性选择元素（ID 在文档中应该是唯一的）。

  ```css
  #element-id {}   /* 选择具有 id="element-id" 的元素 */
  ```

- **属性选择器**：根据 HTML 元素的属性选择元素。

  ```css
  [attribute-name] {}          /* 选择具有指定属性的元素 */
  [attribute-name="value"] {}  /* 选择具有指定属性和值的元素 */
  ```

### 2. 组合选择器

- **后代选择器**（空格分隔符）：选择所有符合条件的后代元素。

  ```css
  div p {}   /* 选择所有 <div> 元素下的所有 <p> 元素 */
  ```

- **直接子元素选择器**（`>` 符号）：选择直接子元素。

  ```css
  ul > li {}   /* 选择所有 <ul> 元素下的直接子元素 <li> */
  ```

- **相邻兄弟元素选择器**（`+` 符号）：选择紧接在另一个元素后的同级元素。

  ```css
  h1 + p {}   /* 选择紧接在 <h1> 元素后的第一个 <p> 元素 */
  ```

- **通用兄弟元素选择器**（`~` 符号）：选择在同一父元素下的所有同级元素。

  ```css
  h1 ~ p {}   /* 选择所有紧接在 <h1> 元素后的所有 <p> 元素 */
  ```

### 3. 伪类和伪元素选择器

- **伪类选择器**：根据元素的状态或位置选择元素，如链接的状态、鼠标悬停等。

  ```css
  a:hover {}      /* 鼠标悬停在链接上时应用的样式 */
  :nth-child(n) {}/* 选择指定位置的子元素 */
  ```

- **伪元素选择器**：用于向某些选择器添加特殊效果，例如 `::before` 和 `::after`。

### 总结

CSS选择器提供了多种灵活和强大的方法来选择页面上的元素，可以根据元素的结构、属性、状态和位置等特征来精确地定位元素并应用样式。选择器的合理使用不仅能提高代码的可维护性和可读性，还能有效地操作和管理页面上的元素。

格式化python
------------

[在 Visual Studio Code (VSCode) 中自动格式化 Python 代码](https://developer.baidu.com/article/details/2925823)

seleniumIDE
-----------

### [安装](https://chromewebstore.google.com/detail/selenium-ide/mooikfkahbdckldjjndioackbalphokd?pli=1)

### [录制操作](https://www.selenium.dev/selenium-ide/docs/en/introduction/getting-started)

### [回放操作](https://www.selenium.dev/selenium-ide/docs/en/introduction/command-line-runner)

下载fnm，再下载npm，node，然后下载selenium-side-runner，最后用·`selenium-side-runner /path/to/your-project.side`来执行录制的操作

fnm
---

#### 安装 Node.js 版本

```
fnm install <version>
```

例如，安装最新的 Node.js 版本：

```
fnm install latest
```

或者安装特定版本：

```
fnm install 14.17.0
```

#### 使用特定版本

```
fnm use <version>
```

例如，使用 14.17.0 版本：

```
fnm use 14.17.0
```

#### 列出已安装的版本

```
fnm list
```

#### 卸载某个版本

```
fnm uninstall <version>
```

#### 设置默认版本

```
fnm default <version>
```

#### 升级 `fnm`

你可以通过重新运行安装脚本来升级 `fnm`：

```
curl -fsSL https://fnm.vercel.app/install | bash
```

小红书爬取
----------

```python
# 要点1 把之前打开的浏览器关掉，程序加载之前的用户配置直接打开浏览器，直接就进入之前的登录状态
# 判断页面是否加载完成，有的时候页面不是同时加载完成的，有多种方法判断页面是否全部加载完成，第一种是等待某个元素加载，第二种是JavaScript的加载状态，第三种是AJAX的请求，结合实际进行选择
# 小红书一些页面使用了动态加载的功能，所以要通过selenium来模拟翻页，然后可以通过页面高度的变化，来判断是否翻到了文件的底部
# csv写入，具体参见代码的74行
import pytest
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys
from time import sleep
import re
import csv

options = webdriver.ChromeOptions()
options.add_argument(r"--start-maximized")
options.add_experimental_option("detach", True)
options.add_argument(
    r"--user-data-dir=C:\Users\Administrator\AppData\Local\Google\Chrome\User Data"
)
options.add_argument(r"--profile-directory=Default")

driver = webdriver.Chrome(options=options)

driver.get(
    "https://www.xiaohongshu.com/user/profile/5c568ea2000000001803e7dc?tab=liked&subTab=note"
)

try:
    previous_height = driver.execute_script("return document.body.scrollHeight")
    page_name = 0
    bangs = []
    while True:
        # 等待页面内容加载完成
        WebDriverWait(driver, 100).until(
   		 lambda d: d.execute_script('return document.readyState') == 'complete'
        )

        parent = driver.find_element(
            by=By.XPATH,
            value='//*[@id="userPageContainer"]/div[3]/div/div[3]/div[1]',
        )
        elements = parent.find_elements(by=By.TAG_NAME, value="section")

        for element in elements:
            content = re.split(r"\r?\n", element.text)
            href = element.find_elements(by=By.TAG_NAME, value="a")[1].get_attribute(
                "href"
            )
            content.append(href)
            if content in bangs:
                continue
            bangs.append(content)
            # bangs = bangs.append(content.append(href))

        """ for element in elements:
            if (
                element.get_attribute("href") != ""
                and element.get_attribute("href") not in bangs
            ):
                bangs.append(element.get_attribute("href")) """
        # 获取当前页面的滚动高度
        actions = ActionChains(driver)
        actions.send_keys(Keys.PAGE_DOWN).perform()
        sleep(2)
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == previous_height:
            break
        previous_height = new_height
    with open("output.csv", "w", newline="", encoding="utf-8") as csvfile:
        csvwriter = csv.writer(csvfile)
        # 写入头部字段
        csvwriter.writerow(["Title", "Author", "Views", "URL"])
        # 写入数据到 CSV 文件
        for row in bangs:
            csvwriter.writerow(row)

finally:
    driver.quit()
```

[微博爬取参阅这个页面](https://blog.csdn.net/poorlytechnology/article/details/109686906)

cookie
------

[作者的思路分成两步，第一步是获取cookie，第二步是利用cookie来进行登录](https://blog.csdn.net/weixin_43821172/article/details/105199481)

### 获取cookie

```python
from selenium import webdriver
import os
import time
import json
 
def browser_initial():
    """"
    进行浏览器初始化
    """
    os.chdir('E:\\pythonwork')
    browser = webdriver.Chrome()
    log_url = 'https://passport.damai.cn/login?ru=https%3A%2F%2Fwww.damai.cn%2F'
    return log_url,browser
 
def get_cookies(log_url,browser):
    """
    获取cookies保存至本地
    """
    browser.get(log_url)
    time.sleep(15)     # 进行扫码
    dictCookies = browser.get_cookies()    # 获取list的cookies
    jsonCookies = json.dumps(dictCookies) #  转换成字符串保存
    
    with open('damai_cookies.txt', 'w') as f:
        f.write(jsonCookies)
    print('cookies保存成功！')
 
if __name__ == "__main__":
    tur = browser_initial()
    get_cookies(tur[0], tur[1])
```

### 利用cookie登录

```python
from selenium import webdriver
import os
import json
 
def browser_initial():
    """"
    浏览器初始化,并打开大麦网购票界面（未登录状态）
    """
    os.chdir('E:\\pythonwork')
    browser = webdriver.Chrome()
    browser.get('https://detail.damai.cn/item.htm?spm=a2oeg.search_category.0.0.8778f91as7xLdc&id=610870234751&clicktitle=2020%E5%BC%A0%E6%9D%B0%E3%80%8C%E6%9C%AA%C2%B7LIVE%E3%80%8D%E5%B7%A1%E5%9B%9E%E6%BC%94%E5%94%B1%E4%BC%9A%20%E5%90%88%E8%82%A5%E7%AB%99')
    return browser
 
def log_damai(browser):
    """
    从本地读取cookies并刷新页面,成为已登录状态
    """
    with open('damai_cookies.txt', 'r', encoding='utf8') as f:
        listCookies = json.loads(f.read())
 
    # 往browser里添加cookies
    for cookie in listCookies:
        cookie_dict = {
            'domain': '.damai.cn',
            'name': cookie.get('name'),
            'value': cookie.get('value'),
            "expires": '',
            'path': '/',
            'httpOnly': False,
            'HostOnly': False,
            'Secure': False
        }
        browser.add_cookie(cookie_dict)
    browser.refresh()                      # 刷新网页,cookies才成功
    
if __name__ == "__main__":
    browser = browser_initial()
    log_damai(browser)
```

[微博爬虫终极版](https://github.com/otonashi-ayana/SpiderWeibo)
---------------------------------------------------------------

这篇文章用request，beautyfly等非常基础的爬虫库，以及MySQL数据库，实现了十万数量级的数据的爬取，我准备深入的研究一下。

selenium grid
-------------

[Selenium Grid快速起步](https://www.selenium.dev/zh-cn/documentation/grid/getting_started/)

Here’s a more complex example of using Selenium Grid with multiple browsers (Chrome and Edge) on a local machine. This script will open both browsers, navigate to a website, perform some actions, and print the page titles.

1. ### **Run the Selenium Grid Hub**:

   ```bash
   java -jar selenium-server-4.22.0.jar hub
   ```

2. ### **Run the Selenium Grid Nodes**:

   ```bash
   java -jar selenium-server-4.22.0.jar node --detect-drivers
   ```

3. ### **Python Script**:

   ```python
   from selenium import webdriver
   from selenium.webdriver.common.desired_capabilities import DesiredCapabilities
   from selenium.webdriver.common.by import By
   from selenium.webdriver.support.ui import WebDriverWait
   from selenium.webdriver.support import expected_conditions as EC
   
   # Define the hub URL
   hub_url = "http://localhost:4444"
   
   # Define the capabilities for Chrome
   chrome_capabilities = DesiredCapabilities.CHROME.copy()
   chrome_capabilities['browserName'] = 'chrome'
   
   # Define the capabilities for Edge
   edge_capabilities = DesiredCapabilities.EDGE.copy()
   edge_capabilities['browserName'] = 'MicrosoftEdge'
   
   # Start a remote WebDriver session for Chrome
   chrome_driver = webdriver.Remote(
       command_executor=hub_url,
       desired_capabilities=chrome_capabilities
   )
   
   # Start a remote WebDriver session for Edge
   edge_driver = webdriver.Remote(
       command_executor=hub_url,
       desired_capabilities=edge_capabilities
   )
   
   def test_browser(driver, url):
       driver.get(url)
       WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
       print(f"Title in {driver.capabilities['browserName']}: {driver.title}")
       # Perform additional actions here
       driver.quit()
   
   # Test with Chrome
   test_browser(chrome_driver, "https://www.selenium.dev")
   
   # Test with Edge
   test_browser(edge_driver, "https://www.selenium.dev")
   
   # Test with multiple pages
   def test_multiple_pages(driver, urls):
       for url in urls:
           driver.get(url)
           WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
           print(f"Title in {driver.capabilities['browserName']} at {url}: {driver.title}")
       driver.quit()
   
   urls_to_test = [
       "https://www.selenium.dev",
       "https://www.google.com",
       "https://www.github.com"
   ]
   
   # Test multiple pages with Chrome
   chrome_driver = webdriver.Remote(
       command_executor=hub_url,
       desired_capabilities=chrome_capabilities
   )
   test_multiple_pages(chrome_driver, urls_to_test)
   
   # Test multiple pages with Edge
   edge_driver = webdriver.Remote(
       command_executor=hub_url,
       desired_capabilities=edge_capabilities
   )
   test_multiple_pages(edge_driver, urls_to_test)
   ```

selenium作用
------------

除了爬虫，Selenium 主要用于以下几个方面：

1. **自动化测试**:
   - Selenium 是一种强大的工具，用于自动化 Web 应用程序的测试。它允许测试人员编写脚本来模拟用户行为并验证应用程序的功能是否正常。
   - 支持多种编程语言（如 Java, Python, C# 等），并且可以与各种测试框架（如 JUnit, TestNG, pytest 等）集成。

2. **回归测试**:
   - 在开发过程中，每次更新代码后都需要重新测试应用程序的功能。Selenium 自动化测试可以在每次代码更改后运行，以确保新代码没有破坏现有功能。

3. **持续集成和持续交付（CI/CD）**:
   - Selenium 可以集成到 CI/CD 流程中，如 Jenkins、GitLab CI、Travis CI 等。当代码提交到版本控制系统时，可以自动运行 Selenium 测试，以确保代码质量和应用程序稳定性。

4. **跨浏览器测试**:
   - Selenium 支持在不同的浏览器（如 Chrome、Firefox、Edge 等）上运行测试，确保应用程序在各种浏览器上都能正常工作。

5. **用户行为模拟**:
   - Selenium 可以模拟用户在浏览器上的操作，如点击、输入文本、导航等，这使得它在功能测试和用户体验测试中非常有用。

通过这些应用，Selenium 不仅提高了测试效率，还减少了手动测试的重复性劳动，使开发团队能够更快、更可靠地发布高质量的软件。

无头浏览器
----------

之前有一个应用广泛的无头浏览器“PhantomJS”，后来谷歌也推出了chrome的无头浏览器，它就走向没落了，在selenium中使用无头浏览器添加以下选项。

```python
options.add_argument("--headless")  # 启用无头模式
options.add_argument("--disable-gpu")  # 禁用 GPU 加速
options.add_argument("--window-size=1920,1080")  # 设置窗口大小
```

资源占用情况
------------

具体的资源消耗会根据实际情况和配置有所不同，但可以给出一些一般性的估计值作为参考：

1. **空间消耗**：
   - **Chrome 浏览器本身**：约 200-300 MB 左右（具体取决于版本和安装选项）。
   - **ChromeDriver**：约 10-20 MB 左右。
   - **总体空间消耗**：大约 250-350 MB，考虑到其他系统和驱动的一些额外消耗。
2. **内存消耗**：
   - **Chrome 浏览器**：加载简单网页时大约消耗 200-300 MB 左右的内存。
   - **复杂网页和大量标签页**：内存消耗可能会增加到 1 GB 或更多，特别是加载大量 JavaScript 和动态内容时。
   - **ChromeDriver 和脚本执行**：通常会增加额外的几十到几百 MB 的内存消耗，具体取决于执行的脚本复杂性和频率。
3. **CPU消耗**：
   - **页面加载和渲染**：在页面加载和渲染期间，Chrome 浏览器会占用一定的 CPU 资源，通常在单个核心上使用 10-20% 的情况比较常见。
   - **JavaScript 执行**：执行复杂 JavaScript 脚本时，CPU 使用率可能会显著增加，高峰时可能达到 50-100% 或更高，取决于脚本复杂性和执行频率。

这些数字仅供参考，具体的资源消耗会因服务器的硬件配置、操作系统负载、运行的脚本复杂性等因素而有所不同。建议在实际部署前进行基准测试，以确保服务器能够满足预期的性能要求和资源消耗。

选项
----

`DesiredCapabilities.CHROME.copy()` 是 Selenium WebDriver 中的一种用法。Selenium 是一个用于自动化 Web 浏览器的工具，而 DesiredCapabilities 则是用来设置 WebDriver 的一些配置选项的。具体来说：

1. **DesiredCapabilities**：这是一个类，用于定义 WebDriver 的各种配置参数。这些参数包括浏览器名称、版本、平台、浏览器的特性等。
2. **CHROME**：这是 DesiredCapabilities 类的一个静态成员，表示用于 Google Chrome 浏览器的默认配置。
3. **copy()**：这个方法用于创建当前配置的一个副本。这在需要在当前配置基础上进行修改时特别有用，而不会影响到原始配置。

webdriver.Remote
----------------

这段代码是使用 Selenium WebDriver 通过远程 WebDriver 实例（通常称为 Selenium Grid）启动浏览器的示例。以下是各个部分的详细解释：

```
pythonCopy codedriver = webdriver.Remote(
    command_executor=hub_url, desired_capabilities=capabilities, options=options
)
```

1. **webdriver.Remote**：
   - 这是 Selenium WebDriver 提供的一个类，用于连接到远程 WebDriver 实例（如 Selenium Grid）。
   - Selenium Grid 允许你将测试分布到多个机器和浏览器上，以实现并行测试，从而加快测试执行速度。
2. **command_executor**：
   - 这是一个参数，用于指定 Selenium Grid Hub 的 URL。Hub 是 Selenium Grid 的控制节点，负责将测试请求分发到合适的节点（Node）上。
   - `hub_url` 是一个变量，包含了 Hub 的 URL 地址，比如 `http://localhost:4444/wd/hub`。
3. **desired_capabilities**：
   - 这是一个参数，用于指定浏览器的期望配置和能力。
   - `capabilities` 是一个包含这些设置的变量。在之前的代码示例中，`capabilities` 是通过 `DesiredCapabilities.CHROME.copy()` 创建的一个配置副本，并可能进行了进一步的自定义。
4. **options**：
   - 这是一个参数，用于传递浏览器的具体选项，比如无头模式、窗口大小、扩展插件等。
   - `options` 是一个变量，通常是 `webdriver.ChromeOptions` 或其他浏览器特定选项类的实例。你可以使用它来进一步配置浏览器的启动选项。

grid启动和driver启动比较
------------------------

**直接使用 Driver**：适用于简单的、本地的测试环境，易于设置和调试，但受限于本地资源和环境依赖。

**通过 Selenium Grid 远程启动**：适用于需要分布式和并行测试的场景，可以充分利用多台机器和多种浏览器环境，但需要更多的配置和管理工作。

爬虫基础
========

爬虫练习网站
------------

### 1. **Web Scraper Academy (webscraper.io)**

Web Scraper 提供了一个虚拟的电子商务网站，专门用于练习爬虫。

- **URL**: Web Scraper Academy

### 2. **Books to Scrape**

一个虚拟的书店网站，数据结构简单，非常适合初学者。

- **URL**: Books to Scrape

### 3. **Quotes to Scrape**

一个虚拟的名言网站，适合练习基本的爬虫技术。

- **URL**: Quotes to Scrape

### 4. **eBay Sandbox**

eBay 提供了一个沙箱环境供开发者测试爬虫和API调用。

- **URL**: eBay Sandbox

### 5. **JSONPlaceholder**

一个免费的 REST API，用于测试和原型制作。

- **URL**: JSONPlaceholder

### 6. **httpbin**

一个提供多种 HTTP 请求和响应测试的网站。

- **URL**: [httpbin](https://httpbin.org/)

### 7. **Data.gov**

美国政府开放数据网站，提供各种格式的数据集，适合进行数据爬取和分析。

- **URL**: [Data.gov](https://www.data.gov/)

### 8. **The Guardian Open Platform**

提供新闻内容的API，可以用来练习从API获取数据。

- **URL**: [The Guardian Open Platform](https://open-platform.theguardian.com/)

### 9. **OpenWeatherMap**

提供天气数据的API，可以用来练习API数据爬取。

- **URL**: OpenWeatherMap

### 10. **Public APIs**

一个收集了各种免费API的网站，可以找到适合爬取的数据源。

- **URL**: [Public APIs](https://public-apis.xyz/)

### 11. **Yelp Fusion API**

提供本地商家和餐厅信息的API，适合练习处理复杂的JSON数据。

- **URL**: Yelp Fusion API

### 12. **CoinGecko API**

提供加密货币市场数据的API，适合练习处理金融数据。

- **URL**: CoinGecko API

### 13. **IMDB datasets**

提供电影和电视节目数据，适合练习大规模数据爬取。

- **URL**: IMDB datasets

### 14. **Kaggle Datasets**

提供各种领域的公开数据集，适合练习数据分析和爬取。

- **URL**: Kaggle Datasets

参考书目
--------

python爬虫开发与项目实战 [对应项目地址](https://github.com/qiyeboy/SpiderBook)

基本库
------

当涉及 Python 中的网络请求和网页抓取时，通常会用到以下几个库：`urllib`、`requests`、`urllib3` 和 `beautifulsoup4`。这些库提供了从简单的HTTP请求到复杂的网页解析和数据提取的功能。

### 1. urllib

`urllib` 是 Python 标准库，提供了处理从 URL 打开数据的模块集合。它包括了：

- **urllib.request**: 发送 HTTP/HTTPS 请求的模块，可以用于打开和读取 URLs。
- **urllib.parse**: 用于解析 URLs。
- **urllib.error**: 包含 urllib.request 引发的异常类。
- **urllib.robotparser**: 解析 robots.txt 文件。

示例使用 `urllib` 发起请求和获取响应：

```python
import urllib.request

url = 'http://example.com'
response = urllib.request.urlopen(url)
html = response.read().decode('utf-8')
print(html)
```

### 2. [requests](https://docs.python-requests.org/en/latest/user/quickstart/#make-a-request)

`requests` 是一个简洁且功能强大的第三方库，用于发送各种 HTTP 请求。它比 `urllib` 更加直观和易用，并且支持更多的高级特性，如会话保持、Cookie 操作等。

示例使用 `requests` 库：

```python
import requests

url = 'http://example.com'
response = requests.get(url)
html = response.text
print(html)
```

### 3. urllib3

`urllib3` 是一个功能强大且线程安全的 HTTP 客户端，它对连接池和重用、线程安全的连接管理以及文件分块上传下载等提供了支持。

示例使用 `urllib3`：

```python
import urllib3

http = urllib3.PoolManager()
url = 'http://example.com'
response = http.request('GET', url)
html = response.data.decode('utf-8')
print(html)
```

### 4. BeautifulSoup

`BeautifulSoup` 是一个用于解析 HTML 和 XML 文档的库，它能够将复杂的 HTML 文档转换为一个树形结构，并提供简单的方法来遍历这棵树和搜索特定元素。

Beautiful Soup支持Python标准库中的HTML解析器，还支持一些第三方的解析器，其中一个是lxml。另一个可供选择的解析器是纯Python实现的html5lib，html5lib的解析方式与浏览器相同

示例使用 `BeautifulSoup` 提取网页内容：

```python
from bs4 import BeautifulSoup
import requests

url = 'http://example.com'
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')
title = soup.title.string
print(f'Title: {title}')
```

知乎示例
--------

```python
# 这个代码可以调用登录知乎然后搜索，但是最后得到的内容是加密的
# 该文探究了知乎解密的方法，“https://blog.csdn.net/qq_36532060/article/details/123899805”

import requests

url = "https://www.zhihu.com/search?type=content&q=%E7%88%AC%E8%99%AB"
user_agent = "Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)"
referer = "http://www.zhihu.com/"
cookies = {
    "_zap": "9e6862e6-0738-4ee7-a899-64a6a0ece802",
    "d_c0": "AACSLERh3BiPTsGnhJlu27i6lPl9Gje08c8=|1719886308",
    "__snaker__id": "88gq68bqfsZAsEK7",
    "q_c1": "e2b1cd9ca9ba4eff90c3abada9b6bbdd|1719886404000|1719886404000",
    "captcha_session_v2": "2|1:0|10:1719886496|18:captcha_session_v2|88:eUJyVHlpUUovQXM4Z0pHZEFIUjFidXhoZmVXQ050V2JjV2VRekQyRkJQVGdwYjBIQTB1dnRnSHVXS0RYaFhkYw==|06bfe5b06cb9292f44d01bf4d47369fa4ca053d329968157dba78d5a755120e0",
    "captcha_ticket_v2": "2|1:0|10:1719886510|17:captcha_ticket_v2|728:eyJ2YWxpZGF0ZSI6IkNOMzFfYndiMEljRy5fRnRKSDJoXypGZmxiRkticVFXUm8zb1VtdzNtaGVnQ3VFb19rY0MwNEl5QjQ5VnMzQk9hVWF0bUpvdThLSTlzQVJ4QmtLa1hoNEVlUFhvSGNKUEIzcmVXMzBUNU9vbTFNdERWWWtPcndfZnNaWnlFWFNTVEgqS0JkS3BTMkVRQVpjTlJsX1VZKkJfRjlGd0pFYUdOVkVCNllhanRmS2lVZ3Bod2lOaWRnd0ZTZFFpYlpfUkt5RnVwYTBiUWRjREl3OThaU1ZVTFV4VGJrVllSWUlvMGs2YVpTUldYQURSQXZKWWZ4eTNlSEFOcENkR1JfQlV1V0MuWkIxeU90MkFtamZwWTBnVGozUzhkS0tJNVlPeV9zVC5uMl8qcm85d09RMFQ1Z08zR3FDbU5HalhWdjY4ZjF0Q2NNZk84U3RXUHFSdF9wWEJDQlRDX0E4Qk5JNHJyWmR6TjBQcU5MbFQ0b2tqTGdhMHhReEVIRkJuS3lJcjgweENTbV8xakVpUG53czVYaWdfb01ZKmFMdzVySGY0MmgudzBFdGR2ZWJIY3I4MVh1dHNyZ1Bka21XajRrd0VYUmN3QWU1cDRxeGsqc2VqdGdtRnpwY2RyeHdpNEhLTXg0dndDRkN5WUdFT0E4LkRlQUR2ZjYyY0RVaTBOLmY0UUo1KjFDLnZiNk03N192X2lfMSJ9|9c5abaf408b34e3ca8b231216683f4677d4329ff8d583b2e519cbd05f296c8f0",
    "z_c0": "2|1:0|10:1719886510|4:z_c0|92:Mi4xNTBZVFJBQUFBQUFBQUpJc1JHSGNHQ1lBQUFCZ0FsVk5yckJ3WndCMFBWT2RWc0RYTzR2dWZ1NHB1VzJNR0FkSkZ3|61c92e1420ac5e173f4e45902dbe2fe8e8aef91bdbf2b8b97a731ff6e2d08ce1",
    "gdxidpyhxdE": "jM3lrVe9vUD%2BPrHH%2Bk9vpPSkyOvAb0W%2Ff7zI4HKZfygtidEqec0XifccWw35RflzoCfDmpdAGRTz3Ab5fJrTT1i2EoWnXhvVQqWMacyN6BfdaZw46HCPpAP1c%2Bj%2FevRjweDjxjiz38DhXSPB%5CzOr8N4M5ahv9CuGC1fPkJSV7p6z%2FB4s%3A1719888058182",
    "_xsrf": "24532abb-7d77-44c4-bb3a-e5a5d51eb698",
    "Hm_lvt_98beee57fd2ef70ccdd5ca52b9740c49": "1719886315,1719977790",
    "Hm_lpvt_98beee57fd2ef70ccdd5ca52b9740c49": "1720064747",
    "tst": "r",
    "SESSIONID": "JUQbKaBBpNxTBYLWeNLY7IusIuIqQbxJTE9F7djtXXh",
    "JOID": "UV4QA0oNGA-kBJJaVQ7vGAZE8IFLQk5bx032GxJ_U3T1R8kTAe8EjcEDlFxRaXylgOx3Wdgw5sTJeIkXj2Paqjg=",
    "osd": "UF4XAE0MGAinA5NaUg3oGQZD84ZKQklYwEz2HBF4UnTyRM4SAegHisADk19WaHyig-t2Wd8z4cXJf4oQjmPdqT8=",
    "KLBRSID": "37f2e85292ebb2c2ef70f1d8e39c2b34|1720064739|1720064739",
    "BEC": "244e292b1eefcef20c9b81b1d9777823",
}

headers = {"User-Agent": user_agent, "Referer": referer}

response = requests.get(url, headers=headers, cookies=cookies)

if response.status_code == 200:
    html = response.text
    with open("index.html", "w", encoding="utf-8") as f:
        f.write(html)
else:
    print("Failed to retrieve data.")
```

beautifulsoup详细用法
---------------------

```python
# find_all的用法和mathematica里面select的用法是类似的，就是从已有的内容了选择出符合条件的内容。可以接受一个判断函数，参见下面具体的判断函数的结构，参数是“tag”， 返回的内容是“true"或者”flase"。
from bs4 import BeautifulSoup
html_str = """
     <html><head><title>The Dormouse's story</title></head>
     <body>
     <p class='title'><b>The Dormouse's story</b></p>
<p class='story'>Once upon a time there were three little sisters; and their names were
     <a href='http://example.com/elsie' class='sister' id='link1'><!-- Elsie --></a>,
     <a href='http://example.com/lacie' class='sister' id='link2'><!-- Lacie --></a> and
     <a href='http://example.com/tillie' class='sister' id='link3'>Tillie</a>;
     and they lived at the bottom of a well.</p>
     <p class='story'>...</p>
     """
soup = BeautifulSoup(html_str, "lxml")
# print(soup.prettify())
def has_selected(tag):
    return tag.has_attr("class") and "sister" in tag.attrs["class"]
selects = soup.find_all(has_selected)
for select in selects:
    print(select.string)
```

lxml例子
--------

```python
# lxml 相比beautifulsoup来说有一个巨大的好处就是支持xpath的选择方式，xpath可以确定一个或者一组符合条件的元素，在选择方面具有很大的优越性
# xpath 参考网站 https://www.w3.org/TR/xpath
from lxml import etree

html_str = """
<html><head><title>The Dormouse's story</title></head>
<body>
<p class='title'><b>The Dormouse's story</b></p>
<p class='story'>Once upon a time there were three little sisters; and their names were
<a href='http://example.com/elsie' class='sister' id='link1'>Elsie</a>,
<a href='http://example.com/lacie' class='sister' id='link2'>Lacie</a> and
<a href='http://example.com/tillie' class='sister' id='link3'>Tillie</a>;
and they lived at the bottom of a well.</p>
<p class='story'>...</p>
"""
html = etree.HTML(html_str)
urls = html.xpath(r"// p[@class='story']/text()")
print(urls)
```

另一个爬取网页的例子

```python
import requests
from lxml import etree
import pprint

url = "https://www.readnovel.com/book/22312481000716402#Catalog"
user_agent = "Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)"
headers = {"User-Agent": user_agent}
response = requests.get(url=url, headers=headers)
html = etree.HTML(response.text)
titles = html.xpath('//*[@id="j-catalogWrap"]/div[2]/div/ul/li/a/text()')
pprint.pprint(titles)
```

json
----

```python
# python 的字典 与 json 的文件 是相对应的
# 首先要创建字典列表，然后类似下面调用json.dump()写入
# 然后可以用json.load()读取，读取出来的内容就是之前存进去的字典的列表

# 这是一个写入的例子
import json

# 假设已经从网页中获取了标题元素列表 titles

# 定义存储结果的列表
results = []
# 遍历每个标题元素
for title in titles:
    # 获取链接和文本内容
    link = title.get('href')  # 获取链接
    text = title.text.strip()  # 获取文本，并去除首尾空格
    # 构建字典对象
    result = {
        'link': link,
        'text': text
    }
    # 将字典对象添加到结果列表中
    results.append(result)

# 将结果列表保存为 JSON 文件
with open('titles.json', 'w', encoding='utf-8') as f:
    json.dump(results, f, ensure_ascii=False, indent=4)

print('保存成功：titles.json')

# 这是一个读取的例子
import json

# 读取 JSON 文件
with open('titles.json', 'r', encoding='utf-8') as f:
    titles_data = json.load(f)

# 打印读取的内容（假设文件结构与保存时相同）
for item in titles_data:
    print(f"标题文本：{item['text']}")
    print(f"链接地址：{item['link']}")
    print()  # 打印空行作为分隔

print("读取完成！")
```

configparser
------------

https://blog.csdn.net/Gscsd_T/article/details/104670205

这篇文档详细介绍了configparser的配置和使用方法

csv
---

```python
# csv 是处理csv文件的模块，之前使用过，python里面对应的对象是列表的列表，或者字典的列表

# 模式1（写入）
import csv
headers = ['ID','UserName','Password','Age','Country']
rows = [(1001,'qiye','qiye_pass',24,'China'),
        (1002,'Mary','Mary_pass',20,'USA'),
        (1003,'Jack','Jack_pass',20,'USA'),
       ]

with open('qiye.csv','w') as f:
    f_csv = csv.writer(f)
    f_csv.writerow(headers)
    f_csv.writerows(rows)
    
# 模式2（写入）
import csv
headers = ['ID','UserName','Password','Age','Country']
rows = [{'ID':1001,'UserName':'qiye','Password':'qiye_pass','Age':24,'Country':' China'},
        {'ID':1002,'UserName':'Mary','Password':'Mary_pass','Age':20,'Country':'USA'},
        {'ID':1003,'UserName':'Jack','Password':'Jack_pass','Age':20,'Country':'USA'},
       ]
with open('qiye.csv','w') as f:
    f_csv = csv.DictWriter(f,headers)
    f_csv.writeheader()
    f_csv.writerows(rows)
    
# 读取
import csv
# 假设 CSV 文件名为 input.csv，编码为 UTF-8
with open('qiye.csv', mode='r', newline='', encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile)
    # 跳过头部字段（如果有的话）
    header = next(csvreader)
    print(f'CSV 头部字段：{header}')
    # 逐行读取数据
    for row in csvreader:
        print(row)
        
def import_urls(file_path):
    urls = []
    with open(file_path, mode="r", newline="", encoding="utf-8") as csvfile:
        csvreader = csv.reader(csvfile)
        for row in csvreader:
            urls.append(row)
    return urls
```

pandas
------

当涉及数据处理和分析时，特别是在 Python 环境中，`pandas` 是一个非常强大和常用的工具库。以下是关于 `pandas` 的一些关键总结：

### 1. 数据结构

- **Series**：一维标记数组，可存储任意数据类型，类似于 Python 的列表或 NumPy 数组。

  ```python
  import pandas as pd
  s = pd.Series([1, 2, 3, 4, 5])
  ```

- **DataFrame**：二维标记数据结构，包含多个列，每列可以是不同的数据类型，类似于 Excel 表格或 SQL 表。

  ```python
  df = pd.DataFrame({
      'Name': ['Alice', 'Bob', 'Charlie'],
      'Age': [25, 30, 35]
  })
  ```

### 2. 基本操作

- **读取和写入数据**：从文件（如 CSV、Excel）、数据库或 URL 读取数据，将数据写入文件。

  ```python
  df = pd.read_csv('data.csv')
  df.to_excel('output.xlsx', index=False)
  ```

- **数据查看和统计**：快速查看数据的前几行、基本统计信息和数据类型。

  ```python
  df.head()
  df.describe()
  df.info()
  ```

### 3. 数据处理

- **选择数据**：选择特定行、列或元素。

  ```python
  df['Column']
  df.iloc[0]
  ```

- **过滤数据**：根据条件过滤数据。

  ```python
  df[df['Age'] > 30]
  ```

- **处理缺失值**：检测、删除或填充缺失的数据。

  ```python
  df.dropna()
  df.fillna(0)
  ```

- **数据转换**：应用函数、映射、分组和排序数据。

  ```python
  df.apply(func)
  df.groupby('Column').mean()
  df.sort_values(by='Column')
  ```

### 4. 数据可视化

- **绘制图表**：生成各种类型的图表，如折线图、柱状图和散点图。

  ```python
  import matplotlib.pyplot as plt
  df.plot(kind='line', x='X', y='Y')
  plt.show()
  ```

### 5. 效率和性能

- **向量化操作**：利用 `pandas` 的向量化操作处理大数据集，比传统循环更高效。

- **内存优化**：使用 `pandas` 的方法来减少内存使用，提升性能。

### 6. 整合与扩展

- **与其他库整合**：与 NumPy、SciPy、Scikit-learn 等科学计算和机器学习库无缝整合。

- **自定义扩展**：通过自定义函数或使用 `apply` 方法扩展 `pandas` 功能。

### 总结

`pandas` 提供了丰富的功能和灵活性，适用于数据的清洗、转换、分析和可视化。通过使用 `pandas`，可以高效地处理结构化数据，从而加快数据分析和建模的速度，是数据科学和分析中不可或缺的工具之一。

urlretrieve
-----------

```python
import urllib
from lxml import etree
import requests
def Schedule(blocknum,blocksize,totalsize):
    '''''
        blocknum:已经下载的数据块
        blocksize:数据块的大小
        totalsize:远程文件的大小
        '''
    per = 100.0 * blocknum * blocksize / totalsize
    if per > 100 :
        per = 100
        print '当前下载进度：%d'%per
user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'
headers={'User-Agent':user_agent}
r = requests.get('http://www.ivsky.com/tupian/ziranfengguang/',headers=headers)
# 使用lxml解析网页
html = etree.HTML(r.text)
img_urls = html.xpath('.// img/@src')# 先找到所有的img
i=0
for img_url in img_urls:
    urllib.urlretrieve(img_url,'img'+str(i)+'.jpg',Schedule)
    i+=1
```



re
--

`re` 模块是 Python 标准库中用于处理正则表达式的模块。它提供了丰富的功能来进行字符串匹配、替换、分割等操作。以下是 `re` 模块的一些常见用法总结：

### 1. 基本匹配操作

#### 1.1 `re.match`

从字符串的开头进行匹配。

```python
import re

text = "Hello, world!"
pattern = re.compile(r'(\w+), (\w+)!')

# 匹配文本
match = pattern.match(text)

if match:
    print("匹配的字符串:", match.group())   # 输出: "Hello, world!"
    print("第一个捕获组:", match.group(1))  # 输出: "Hello"
    print("第二个捕获组:", match.group(2))  # 输出: "world"
    print("匹配起始位置:", match.start())   # 输出: 0
    print("匹配结束位置:", match.end())     # 输出: 13
    print("匹配位置范围:", match.span())    # 输出: (0, 13)
    print("原始字符串:", match.string)     # 输出: "Hello, world!"
    print("正则表达式对象:", match.re)     # 输出: re.compile(r'(\w+), (\w+)!')
```

#### 1.2 `re.search`

扫描整个字符串并返回第一个匹配。

```python
search = re.search(pattern, text)
if search:
    print('匹配成功:', search.group())
else:
    print('匹配失败')
```

#### 1.3 `re.findall`

返回所有非重叠的匹配。

```python
text = 'hello world, hello universe'
all_matches = re.findall(pattern, text)
print('所有匹配:', all_matches)
```

#### 1.4 `re.finditer`

返回一个迭代器，迭代器包含所有匹配的 `match` 对象。

```python
iterator = re.finditer(pattern, text)
for match in iterator:
    print('匹配成功:', match.group())
```

### 2. 替换和分割

#### 2.1 `re.sub`

替换所有匹配的子串。

```python
text = 'hello world, hello universe'
new_text = re.sub(pattern, 'hi', text)
print('替换后的文本:', new_text)
```

#### 2.2 `re.subn`

替换所有匹配的子串，并返回替换次数。

```python
new_text, num_replacements = re.subn(pattern, 'hi', text)
print('替换后的文本:', new_text)
print('替换次数:', num_replacements)
```

#### 2.3 `re.split`

根据匹配分割字符串。

```python
text = 'one1two2three3four'
pattern = r'\d'
split_text = re.split(pattern, text)
print('分割结果:', split_text)
```

### 3. 编译正则表达式

为了提高效率，可以将正则表达式编译成模式对象。

```python
pattern = re.compile(r'hello')

# 使用编译后的模式对象进行匹配
match = pattern.match('hello world')
if match:
    print('匹配成功:', match.group())
```

### 4. 特殊字符和转义

正则表达式中的特殊字符需要转义，例如 `.`，`*`，`?`，`\` 等。

```python
pattern = re.compile(r'\d+')  # 匹配一个或多个数字
text = 'There are 123 apples and 456 oranges'
matches = pattern.findall(text)
print('匹配的数字:', matches)
```

### 5. 分组和捕获

使用括号 `()` 创建分组并捕获匹配的子串。

```python
pattern = re.compile(r'(\d+)-(\d+)-(\d+)')
text = 'Phone number: 123-456-7890'

match = pattern.search(text)
if match:
    print('完整匹配:', match.group())
    print('第一个分组:', match.group(1))
    print('第二个分组:', match.group(2))
    print('第三个分组:', match.group(3))
```

### 6. 非捕获分组和命名分组

#### 6.1 非捕获分组

使用 `(?:...)` 创建非捕获分组。

```python
pattern = re.compile(r'(?:\d+)-(\d+)-(\d+)')
text = 'Phone number: 123-456-7890'

match = pattern.search(text)
if match:
    print('完整匹配:', match.group())
    print('第一个分组:', match.group(1))
    print('第二个分组:', match.group(2))
```

#### 6.2 命名分组

使用 `(?P<name>...)` 创建命名分组。

```python
pattern = re.compile(r'(?P<area>\d+)-(?P<exchange>\d+)-(?P<number>\d+)')
text = 'Phone number: 123-456-7890'

match = pattern.search(text)
if match:
    print('完整匹配:', match.group())
    print('区号:', match.group('area'))
    print('交换号:', match.group('exchange'))
    print('号码:', match.group('number'))
```

### 7. 常见正则表达式模式

- `.`: 匹配任意字符（除换行符）。
- `\d`: 匹配任何数字，相当于 `[0-9]`。
- `\D`: 匹配任何非数字字符。
- `\w`: 匹配任何字母数字字符，相当于 `[a-zA-Z0-9_]`。
- `\W`: 匹配任何非字母数字字符。
- `\s`: 匹配任何空白字符（空格、制表符、换行符）。
- `\S`: 匹配任何非空白字符。
- `*`: 匹配前面的字符零次或多次。
- `+`: 匹配前面的字符一次或多次。
- `?`: 匹配前面的字符零次或一次。
- `{n}`: 匹配前面的字符恰好 n 次。
- `{n,}`: 匹配前面的字符至少 n 次。
- `{n,m}`: 匹配前面的字符至少 n 次，至多 m 次。
- `^`: 匹配字符串的开头。
- `$`: 匹配字符串的结尾。
- `[]`: 字符集，匹配方括号内的任意字符。

### 示例总结

以下是一个综合示例，展示如何使用 `re` 模块进行各种操作：

```python
import re

# 定义文本
text = "My email is example@example.com and my phone number is 123-456-7890."

# 匹配电子邮件
email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
email_match = re.search(email_pattern, text)
if email_match:
    print('找到的电子邮件:', email_match.group())

# 匹配电话号码
phone_pattern = r'(\d{3})-(\d{3})-(\d{4})'
phone_match = re.search(phone_pattern, text)
if phone_match:
    print('找到的电话号码:', phone_match.group())
    print('区号:', phone_match.group(1))
    print('交换号:', phone_match.group(2))
    print('号码:', phone_match.group(3))

# 替换所有数字
text_with_replacement = re.sub(r'\d', '#', text)
print('替换后的文本:', text_with_replacement)

# 根据匹配分割文本
split_text = re.split(r'\s+', text)
print('分割后的文本:', split_text)
```

这些是 `re` 模块的一些常见用法，涵盖了基本匹配、替换和分割操作、分组和捕获等。通过 `re` 模块，你可以方便地进行各种复杂的字符串处理操作。

pillow
------

Pillow是Python的一个图像处理库，基于PIL（Python Imaging Library）库，可以用于打开、操作和保存许多不同格式的图像。以下是Pillow的基本用法总结：

### 安装
首先，需要安装Pillow库：
```sh
pip install Pillow
```

### 打开图像
使用`Image`模块打开图像：
```python
from PIL import Image

# 打开图像文件
image = Image.open('example.jpg')
```

### 显示图像
使用`show`方法显示图像：
```python
image.show()
```

### 保存图像
将图像保存到文件中：
```python
image.save('output.png')
```

### 图像格式转换
将图像转换为不同的格式：
```python
image.save('output.png', 'PNG')
```

### 获取图像信息
获取图像的基本信息，如格式、尺寸和模式：
```python
print(image.format)  # 图像格式，例如 'JPEG'
print(image.size)    # 图像尺寸，例如 (1920, 1080)
print(image.mode)    # 图像模式，例如 'RGB'
```

### 图像缩放和旋转
图像缩放：
```python
# 缩放图像到指定尺寸
resized_image = image.resize((800, 600))
```

图像旋转：
```python
# 旋转图像90度
rotated_image = image.rotate(90)
```

### 剪裁图像
剪裁图像的一部分：
```python
# 定义剪裁区域 (左, 上, 右, 下)
crop_area = (100, 100, 400, 400)
cropped_image = image.crop(crop_area)
```

### 调整图像颜色
转换图像为灰度图像：
```python
gray_image = image.convert('L')
```

### 处理图像像素
访问和修改图像像素：
```python
# 获取图像的像素数据
pixels = image.load()

# 获取指定像素的颜色值
color = pixels[0, 0]

# 修改指定像素的颜色值
pixels[0, 0] = (255, 0, 0)  # 将左上角像素设置为红色
```

### 绘图
在图像上绘制图形或文本：
```python
from PIL import ImageDraw, ImageFont

# 创建ImageDraw对象
draw = ImageDraw.Draw(image)

# 绘制矩形
draw.rectangle([50, 50, 150, 150], outline="red", width=3)

# 绘制文本
font = ImageFont.truetype("arial.ttf", 36)
draw.text((50, 50), "Hello, Pillow", font=font, fill="white")
```

### 滤镜和效果
应用滤镜和效果：
```python
from PIL import ImageFilter

# 应用模糊滤镜
blurred_image = image.filter(ImageFilter.BLUR)
```

以上是Pillow库的一些基本用法示例，Pillow还提供了更多高级功能和操作，可以查阅Pillow的官方文档获取详细信息。

os
--

`os` 模块是 Python 标准库中一个用于与操作系统交互的模块。它提供了丰富的功能来处理文件和目录、管理进程、获取环境变量等。以下是 `os` 模块的一些常见用法总结：

### 1. 文件和目录操作

#### 1.1 获取当前工作目录

```python
import os

current_dir = os.getcwd()
print(current_dir)
```

#### 1.2 改变当前工作目录

```python
os.chdir('/path/to/directory')
```

#### 1.3 列出目录中的文件和子目录

```python
files_and_dirs = os.listdir('/path/to/directory')
print(files_and_dirs)
```

#### 1.4 创建目录

```python
os.mkdir('new_directory')  # 创建一个新目录
os.makedirs('parent_directory/new_directory')  # 递归创建目录
```

#### 1.5 删除目录

```python
os.rmdir('directory_to_remove')  # 删除一个空目录
os.removedirs('parent_directory/new_directory')  # 递归删除目录
```

#### 1.6 删除文件

```python
os.remove('file_to_remove.txt')
```

#### 1.7 重命名文件或目录

```python
os.rename('old_name.txt', 'new_name.txt')
```

### 2. 文件和目录路径操作

#### 2.1 获取文件或目录的绝对路径

```python
absolute_path = os.path.abspath('relative_path.txt')
print(absolute_path)
```

#### 2.2 分割路径

```python
directory, filename = os.path.split('/path/to/file.txt')
print(directory)  # 输出: /path/to
print(filename)  # 输出: file.txt
```

#### 2.3 分割文件扩展名

```python
filename, file_extension = os.path.splitext('/path/to/file.txt')
print(filename)  # 输出: /path/to/file
print(file_extension)  # 输出: .txt
```

#### 2.4 连接路径

```python
full_path = os.path.join('/path/to', 'file.txt')
print(full_path)  # 输出: /path/to/file.txt
```

### 3. 环境变量

#### 3.1 获取环境变量

```python
path_variable = os.getenv('PATH')
print(path_variable)
```

#### 3.2 设置环境变量

```python
os.environ['NEW_VARIABLE'] = 'value'
```

### 4. 执行系统命令

```python
os.system('ls -l')
```

### 5. 获取文件或目录的属性

#### 5.1 检查文件或目录是否存在

```python
exists = os.path.exists('/path/to/file_or_directory')
print(exists)  # 输出: True 或 False
```

#### 5.2 检查路径是否是文件

```python
is_file = os.path.isfile('/path/to/file')
print(is_file)  # 输出: True 或 False
```

#### 5.3 检查路径是否是目录

```python
is_dir = os.path.isdir('/path/to/directory')
print(is_dir)  # 输出: True 或 False
```

#### 5.4 获取文件大小

```python
file_size = os.path.getsize('/path/to/file')
print(file_size)  # 输出: 文件大小（以字节为单位）
```

### 6. 进程管理

#### 6.1 获取当前进程ID

```python
pid = os.getpid()
print(pid)
```

#### 6.2 获取父进程ID

```python
ppid = os.getppid()
print(ppid)
```

### 示例总结

以下是一个综合示例，展示如何使用 `os` 模块进行文件和目录操作：

```python
import os

# 获取当前工作目录
current_dir = os.getcwd()
print("当前工作目录:", current_dir)

# 列出当前目录中的文件和子目录
files_and_dirs = os.listdir(current_dir)
print("当前目录中的文件和子目录:", files_and_dirs)

# 创建一个新目录
os.mkdir('new_directory')
print("创建 new_directory 目录")

# 创建一个多级目录
os.makedirs('parent_directory/new_directory')
print("创建 parent_directory/new_directory 目录")

# 删除一个空目录
os.rmdir('new_directory')
print("删除 new_directory 目录")

# 删除一个多级目录
os.removedirs('parent_directory/new_directory')
print("删除 parent_directory/new_directory 目录")

# 创建一个文件并写入内容
with open('example.txt', 'w') as f:
    f.write('Hello, World!')

# 检查文件是否存在
if os.path.exists('example.txt'):
    print("example.txt 文件存在")

# 获取文件大小
file_size = os.path.getsize('example.txt')
print("example.txt 文件大小:", file_size)

# 重命名文件
os.rename('example.txt', 'renamed_example.txt')
print("example.txt 重命名为 renamed_example.txt")

# 删除文件
os.remove('renamed_example.txt')
print("删除 renamed_example.txt 文件")
```

这些是 `os` 模块的一些常见用法，涵盖了文件和目录操作、路径操作、环境变量管理、进程管理等。通过 `os` 模块，你可以方便地与操作系统进行交互，执行各种文件系统和进程相关的操作。

subprocess 和 multiprocessing
-----------------------------

**`subprocess`**：适用于需要启动和管理外部进程（例如运行系统命令或其他脚本）的时候。

**`multiprocessing`**：适用于需要并行执行 Python 代码并共享数据的情况。

```python
# subprocess
import subprocess

process = subprocess.Popen(['python', 'Spiderwork'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
stdout, stderr = process.communicate()

print("标准输出：", stdout.decode())
print("标准错误：", stderr.decode())


# multiprocessing
from multiprocessing import Process, Value, Array

def worker(num, arr):
    num.value = 3.1415927
    for i in range(len(arr)):
        arr[i] = -arr[i]

if __name__ == '__main__':
    num = Value('d', 0.0)
    arr = Array('i', range(10))

    p = Process(target=worker, args=(num, arr))
    p.start()
    p.join()

    print(num.value)
    print(arr[:])
```



爬虫基础模块
------------

基础爬虫框架主要包括五大模块，分别为爬虫调度器、URL管理器、HTML下载器、HTML解析器、数据存储器。功能分析如下：

- 爬虫调度器主要负责统筹其他四个模块的协调工作。

- URL管理器负责管理URL链接，维护已经爬取的URL集合和未爬取的URL集合，提供获取新URL链接的接口。

- HTML下载器用于从URL管理器中获取未爬取的URL链接并下载HTML网页。

- HTML解析器用于从HTML下载器中获取已经下载的HTML网页，并从中解析出新的URL链接交给URL管理器，解析出有效数据交给数据存储器。

- 数据存储器用于将HTML解析器解析出来的数据通过文件或者数据库的形式存储起来。

```python
# coding:utf-8
import requests
import re
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import codecs
import csv


class UrlManager:
    def __init__(self):
        self.new_urls = set()  # 未爬取URL集合
        self.old_urls = set()  # 已爬取URL集合

    def has_new_url(self):
        """
        判断是否有未爬取的URL
        """
        return self.new_url_size() != 0

    def get_new_url(self):
        """
        获取一个未爬取的URL
        """
        new_url = self.new_urls.pop()
        self.old_urls.add(new_url)
        return new_url

    def add_new_url(self, url):
        """
        将新的URL添加到未爬取的URL集合中
        :param url: 单个URL
        """
        if url is None:
            return
        if url not in self.new_urls and url not in self.old_urls:
            self.new_urls.add(url)

    def add_new_urls(self, urls):
        """
        将新的URL添加到未爬取的URL集合中
        :param urls: url集合
        """
        if urls is None or len(urls) == 0:
            return
        for url in urls:
            self.add_new_url(url)

    def new_url_size(self):
        """
        获取未爬取URL集合的大小
        """
        return len(self.new_urls)

    def old_url_size(self):
        """
        获取已经爬取URL集合的大小
        """
        return len(self.old_urls)


class HtmlDownloader:

    def download(self, url):
        if url is None:
            return None

        user_agent = "Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)"
        headers = {"User-Agent": user_agent}

        try:
            r = requests.get(url, headers=headers)
            r.raise_for_status()  # 检查请求是否成功
            r.encoding = "utf-8"
            return r.text
        except requests.RequestException as e:
            print(f"Error downloading {url}: {e}")
            return None


class HtmlParser:

    def parser(self, page_url, html_cont):
        """
        用于解析网页内容，抽取URL和数据
        :param page_url: 下载页面的URL
        :param html_cont: 下载的网页内容
        :return: 返回URL和数据
        """
        if page_url is None or html_cont is None:
            return None, None

        soup = BeautifulSoup(html_cont, "html.parser")
        new_urls = self._get_new_urls(page_url, soup)
        new_data = self._get_new_data(page_url, soup)
        return new_urls, new_data

    def select_method(self, tag):
        return (
            tag.name == "a"
            and "#" not in tag.get("href", "")
            and "item" in tag.get("href", "")
        )

    def _get_new_urls(self, page_url, soup):
        """
        抽取新的URL集合
        :param page_url: 下载页面的URL
        :param soup: soup对象
        :return: 返回新的URL集合
        """
        new_urls = set()
        # 抽取符合要求的a标记
        links = soup.find_all(self.select_method)
        for link in links:
            # 提取href属性
            if link and link.has_attr("href"):
                new_url = link["href"]
                new_full_url = "http://baike.baidu.com" + new_url
                new_urls.add(new_full_url)
        return new_urls

    def _get_new_data(self, page_url, soup):
        """
        抽取有效数据
        :param page_url: 下载页面的URL
        :param soup: soup对象
        :return: 返回有效数据
        """
        data = {}
        data["url"] = page_url
        title_node = soup.find("h1", class_="J-lemma-title")
        print(title_node.get_text())
        data["title"] = title_node.get_text() if title_node else ""
        summary_node = soup.find("div", attrs={"class": "MARK_MODULE"})
        data["summary"] = summary_node.get_text() if summary_node else ""
        return data


class DataOutput:
    def __init__(self):
        self.datas = []

    def store_data(self, data):
        if data is None:
            return
        self.datas.append(data)

    def output_csv(self):
        headers = ["title", "summary", "url"]
        with open("data.csv", "w", encoding="utf-8") as f:
            csvfile = csv.DictWriter(f, headers)
            csvfile.writeheader()
            csvfile.writerows(self.datas)

    def output_html(self):
        with open("郑州.md", "w", encoding="utf-8") as f:
            f.write("## 郑州\r\n")
            for data in self.datas:
                f.write(
                    "### " + "[" + data["title"] + "]" + "(" + data["url"] + ")\r\n"
                )
                f.write(data["summary"] + "\r\n")


class SpiderMan:
    def __init__(self, target):
        self.target = target
        self.manager = UrlManager()
        self.downloader = HtmlDownloader()
        self.parser = HtmlParser()
        self.output = DataOutput()

    def crawl(self, root_url):
        # 添加入口URL
        self.manager.add_new_url(root_url)
        # 判断url管理器中是否有新的url，同时判断抓取了多少个url
        while self.manager.has_new_url() and self.manager.old_url_size() < 100:
            try:
                # 从URL管理器获取新的url
                new_url = self.manager.get_new_url()
                # HTML下载器下载网页
                html = self.downloader.download(new_url)
                # HTML解析器抽取网页数据
                new_urls, data = self.parser.parser(new_url, html)
                # 将抽取的url添加到URL管理器中
                self.manager.add_new_urls(new_urls)
                # 数据存储器存储文件
                self.output.store_data(data)
                print(f"已经抓取{self.manager.old_url_size()}个链接")
            except Exception as e:
                print(f"crawl failed: {e}")
        # 数据存储器将文件输出成指定格式
        self.output.output_html()


if __name__ == "__main__":
    spider_man = SpiderMan("")
    spider_man.crawl("https://baike.baidu.com/item/%E9%83%91%E5%B7%9E%E5%B8%82/2439317")
```

分布式爬虫
----------

### 节点管理器

```python
# coding:utf-8
from multiprocessing.managers import BaseManager
import time
from multiprocessing import Process, Queue
from DataOutput import DataOutput
from UrlManager import UrlManager

class NodeManager(object):

    def start_Manager(self, url_q, result_q):
        """
        创建一个分布式管理器
        :param url_q: url队列
        :param result_q: 结果队列
        :return:
        """
        # 把创建的两个队列注册在网络上，利用register方法，callable参数关联了Queue对象，
        # 将Queue对象在网络中暴露
        BaseManager.register("get_task_queue", callable=lambda: url_q)
        BaseManager.register("get_result_queue", callable=lambda: result_q)
        # 绑定端口8001，设置验证口令‘baike’。这个相当于对象的初始化
        manager = BaseManager(address=("", 8001), authkey="baike".encode("utf-8"))
        # 返回manager对象
        return manager

    def url_manager_proc(self, url_q, conn_q, root_url):
        url_manager = UrlManager()
        url_manager.add_new_url(root_url)
        while True:
            while url_manager.has_new_url():
                # 从URL管理器获取新的url
                new_url = url_manager.get_new_url()
                # 将新的URL发给工作节点
                url_q.put(new_url)
                print("old_url=", url_manager.old_url_size())
                # 加一个判断条件，当爬去2000个链接后就关闭,并保存进度
                if url_manager.old_url_size() > 2000:
                    # 通知爬行节点工作结束
                    url_q.put("end")
                    print("控制节点发起结束通知!")
                    # 关闭管理节点，同时存储set状态
                    url_manager.save_progress("new_urls.txt", url_manager.new_urls)
                    url_manager.save_progress("old_urls.txt", url_manager.old_urls)
                    return
            # 将从result_solve_proc获取到的urls添加到URL管理器之间
            try:
                urls = conn_q.get()
                url_manager.add_new_urls(urls)
            except BaseException as e:
                time.sleep(0.1)  # 延时休息

    def result_solve_proc(self, result_q, conn_q, store_q):
        while True:
            try:
                if not result_q.empty():
                    # Queue.get(block=True, timeout=None)
                    content = result_q.get(True)
                    if content["new_urls"] == "end":
                        # 结果分析进程接受通知然后结束
                        print("结果分析进程接受通知然后结束!")
                        store_q.put("end")
                        return
                    conn_q.put(content["new_urls"])  # url为set类型
                    store_q.put(content["data"])  # 解析出来的数据为dict类型
                else:
                    time.sleep(0.1)  # 延时休息
            except BaseException as e:
                time.sleep(0.1)  # 延时休息

    def store_proc(self, store_q):
        output = DataOutput()
        while True:
            if not store_q.empty():
                data = store_q.get()
                if data == "end":
                    print("存储进程接受通知然后结束!")
                    output.ouput_end(output.filepath)

                    return
                output.store_data(data)
            else:
                time.sleep(0.1)
        pass


if __name__ == "__main__":
    # 初始化4个队列

    url_q = Queue()
    result_q = Queue()
    store_q = Queue()
    conn_q = Queue()
    # 创建分布式管理器
    node = NodeManager()
    manager = node.start_Manager(url_q, result_q)
    # 创建URL管理进程、 数据提取进程和数据存储进程
    url_manager_proc = Process(
        target=node.url_manager_proc,
        args=(
            url_q,
            conn_q,
            "http://baike.baidu.com/view/284853.htm",
        ),
    )
    result_solve_proc = Process(
        target=node.result_solve_proc,
        args=(
            result_q,
            conn_q,
            store_q,
        ),
    )
    store_proc = Process(target=node.store_proc, args=(store_q,))
    # 启动3个进程和分布式管理器
    url_manager_proc.start()
    result_solve_proc.start()
    store_proc.start()
    manager.get_server().serve_forever()
```

### 节点工作系统

```python
# coding:utf-8
from multiprocessing.managers import BaseManager

from HtmlDownloader import HtmlDownloader
from HtmlParser import HtmlParser


class SpiderWork(object):
    def __init__(self):
        # 初始化分布式进程中的工作节点的连接工作
        # 实现第一步：使用BaseManager注册获取Queue的方法名称
        BaseManager.register("get_task_queue")
        BaseManager.register("get_result_queue")
        # 实现第二步：连接到服务器:
        server_addr = "127.0.0.1"
        print(("Connect to server %s..." % server_addr))
        # 端口和验证口令注意保持与服务进程设置的完全一致:
        self.m = BaseManager(
            address=(server_addr, 8001), authkey="baike".encode("utf-8")
        )
        # 从网络连接:
        self.m.connect()
        # 实现第三步：获取Queue的对象:
        self.task = self.m.get_task_queue()
        self.result = self.m.get_result_queue()
        # 初始化网页下载器和解析器
        self.downloader = HtmlDownloader()
        self.parser = HtmlParser()
        print("init finish")

    def crawl(self):
        while True:
            try:
                if not self.task.empty():
                    url = self.task.get()

                    if url == "end":
                        print("控制节点通知爬虫节点停止工作...")
                        # 接着通知其它节点停止工作
                        self.result.put({"new_urls": "end", "data": "end"})
                        return
                    print("爬虫节点正在解析:%s" % url.encode("utf-8"))
                    content = self.downloader.download(url)
                    new_urls, data = self.parser.parser(url, content)
                    self.result.put({"new_urls": new_urls, "data": data})
            except EOFError as e:
                print("连接工作节点失败")
                return
            except Exception as e:
                print(e)
                print("Crawl  fali ")


if __name__ == "__main__":
    spider = SpiderWork()
    spider.crawl()
```

爬虫进阶
========

python配置pip源
---------------

使用中国的镜像源可以加快安装Python包的速度。以下是一些常用的中国镜像源：

1. **清华大学镜像源**

   ```
   https://pypi.tuna.tsinghua.edu.cn/simple
   ```

2. **阿里云镜像源**

   ```
   https://mirrors.aliyun.com/pypi/simple
   ```

3. **中国科技大学镜像源**

   ```
   https://pypi.mirrors.ustc.edu.cn/simple
   ```

4. **豆瓣镜像源**

   ```
   https://pypi.douban.com/simple
   ```

5. **腾讯云镜像源**

   ```
   https://mirrors.cloud.tencent.com/pypi/simple
   ```

### 临时使用镜像源

你可以在使用`pip`安装包时通过命令行参数指定镜像源，例如：

```
pip install <package-name> -i https://pypi.tuna.tsinghua.edu.cn/simple
```

晋江网
------

安装浏览器对应的driver

[安装ocr工具](https://github.com/UB-Mannheim/tesseract/releases/download/v5.4.0.20240606/tesseract-ocr-w64-setup-5.4.0.20240606.exe)

安装python

pip install selenium pillow pytesseract -i https://pypi.tuna.tsinghua.edu.cn/simple

修改代码中的配置文件，然后运行调试

scrapy
------

scrapy是一个强大的爬虫框架，但是比较难学。注意它本身还是python，只是集成度和专业性比较高，但是基本的python 的用法在这里面还都是适用的。

他有几个模块一起构成，一个是爬虫模块，在这块它主要执行爬虫的操作，它获取目标网页，然后进行解析，返回爬取的内容以及将要爬取的其他的url。第二个模块是储存模块，这块是一个叫item.py的文件，他把item定义为一个通道，可以存入和索引内容，就相当于`multiprogressing`里面的`Queue`这个模块，但是它做了一个集成和封装。

使用 Scrapy 爬虫的基本步骤包括以下几部分：

1. **安装 Scrapy**：确保你已经安装了 Scrapy。
2. **创建一个新的 Scrapy 项目**：使用 Scrapy 提供的命令行工具来创建一个新的项目。
3. **定义 Item**：定义你要抓取的数据结构。
4. **编写 Spider**：编写一个 Spider 来定义爬取的逻辑。
5. **配置 Pipelines**：设置数据存储管道（如 MongoDB）。
6. **运行 Spider**：执行爬虫来抓取数据。

### 分布式爬虫

![image-20240718172211999](https://raw.githubusercontent.com/Cipivious/my_try/main/img/image-20240718172211999.png)

验证码
------

### 打码平台

[斐斐打码](http://www.fateadm.com/)

数据库
------

### 安装sqlite3[数据库](https://www.sqlite.org/download.html)

下载对应的库以及可执行文件，用于后续直接或者间接使用

### mongod数据库

[安装mongod数据库](https://www.mongodb.com/try/download/community)

[安装mongodsh](https://www.mongodb.com/try/download/shell)

上面是数据库的服务器，而后面是数据库的操作工具

文件加密
--------

```javascript
var _0xb483 = ["\x5F\x64\x65\x63\x6F\x64\x65", "\x68\x74\x74\x70\x3A\x2F\x2F\x77\x77\x77\x2E\x73\x6F\x6A\x73\x6F\x6E\x2E\x63\x6F\x6D\x2F\x6A\x61\x76\x61\x73\x63\x72\x69\x70\x74\x6F\x62\x66\x75\x73\x63\x61\x74\x6F\x72\x2E\x68\x74\x6D\x6C"];
(function(_0xd642x1) {
    _0xd642x1[_0xb483[0]] = _0xb483[1]
}
)(window);
var __Ox2133f = ["\x75\x73\x65\x20\x73\x74\x72\x69\x63\x74", "\x24", "\x53\x70\x61\x72\x6B\x4D\x44\x35", "\x63\x68\x61\x72\x43\x6F\x64\x65\x41\x74", "\x6C\x65\x6E\x67\x74\x68", "\x73\x75\x62\x73\x74\x72\x69\x6E\x67", "\x6D\x61\x74\x63\x68", "\x73\x75\x62\x61\x72\x72\x61\x79", "\x30", "\x31", "\x32", "\x33", "\x34", "\x35", "\x36", "\x37", "\x38", "\x39", "\x61", "\x62", "\x63", "\x64", "\x65", "\x66", "", "\x6A\x6F\x69\x6E", "\x72\x65\x73\x65\x74", "\x68\x65\x6C\x6C\x6F", "\x35\x64\x34\x31\x34\x30\x32\x61\x62\x63\x34\x62\x32\x61\x37\x36\x62\x39\x37\x31\x39\x64\x39\x31\x31\x30\x31\x37\x63\x35\x39\x32", "\x61\x70\x70\x65\x6E\x64", "\x70\x72\x6F\x74\x6F\x74\x79\x70\x65", "\x74\x65\x73\x74", "\x61\x70\x70\x65\x6E\x64\x42\x69\x6E\x61\x72\x79", "\x5F\x62\x75\x66\x66", "\x5F\x6C\x65\x6E\x67\x74\x68", "\x73\x75\x62\x73\x74\x72", "\x65\x6E\x64", "\x5F\x73\x74\x61\x74\x65", "\x5F\x66\x69\x6E\x69\x73\x68", "\x64\x65\x73\x74\x72\x6F\x79", "\x68\x61\x73\x68", "\x68\x61\x73\x68\x42\x69\x6E\x61\x72\x79", "\x41\x72\x72\x61\x79\x42\x75\x66\x66\x65\x72", "\x62\x79\x74\x65\x4C\x65\x6E\x67\x74\x68", "\x5F\x63\x6F\x6E\x63\x61\x74\x41\x72\x72\x61\x79\x42\x75\x66\x66\x65\x72", "\x73\x65\x74", "\x63\x39\x64\x36\x36\x31\x38\x64\x62\x63\x36\x35\x37\x62\x34\x31\x61\x36\x36\x65\x62\x30\x61\x66\x39\x35\x32\x39\x30\x36\x66\x31", "\x6E\x61\x6D\x65", "\x73\x6C\x69\x63\x65", "\x63\x61\x6C\x6C", "\x74\x6F\x53\x74\x72\x69\x6E\x67", "\x4F\x62\x6A\x65\x63\x74", "\x76\x61\x6C\x75\x65", "\x41\x72\x72\x61\x79", "\x4E\x75\x6C\x6C", "\x55\x6E\x64\x65\x66\x69\x6E\x65\x64", "\x6D\x61\x70", "\x70\x75\x73\x68", "\x73\x6F\x72\x74", "\x66\x6F\x72\x45\x61\x63\x68", "\x73\x74\x72\x69\x6E\x67\x69\x66\x79", "\x64\x61\x74\x61", "\x65\x78\x74\x65\x6E\x64", "\x5F\x73\x6E", "\x5F\x74\x73", "\x67\x65\x74\x54\x69\x6D\x65", "\x26\x5F\x74\x73\x3D", "\x26\x5F\x73\x6E\x3D", "\x5F\x74\x73\x3D", "\x61\x6A\x61\x78\x50\x72\x65\x66\x69\x6C\x74\x65\x72"];
(function() {
    __Ox2133f[0];
    var _0xe7fex1 = window[__Ox2133f[1]];
    var _0xe7fex2 = (window[__Ox2133f[2]] = (function() {
        __Ox2133f[0];
        var _0xe7fex3 = function(_0xe7fex12, _0xe7fex13) {
            return (_0xe7fex12 + _0xe7fex13) & 0xffffffff
        }
          , _0xe7fex4 = function(_0xe7fex14, _0xe7fex12, _0xe7fex13, _0xe7fex15, _0xe7fex16, _0xe7fex17) {
            _0xe7fex12 = _0xe7fex3(_0xe7fex3(_0xe7fex12, _0xe7fex14), _0xe7fex3(_0xe7fex15, _0xe7fex17));
            return _0xe7fex3((_0xe7fex12 << _0xe7fex16) | (_0xe7fex12 >>> (32 - _0xe7fex16)), _0xe7fex13)
        }
          , _0xe7fex5 = function(_0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex15, _0xe7fex16, _0xe7fex17) {
            return _0xe7fex4((_0xe7fex13 & _0xe7fex18) | (~_0xe7fex13 & _0xe7fex19), _0xe7fex12, _0xe7fex13, _0xe7fex15, _0xe7fex16, _0xe7fex17)
        }
          , _0xe7fex6 = function(_0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex15, _0xe7fex16, _0xe7fex17) {
            return _0xe7fex4((_0xe7fex13 & _0xe7fex19) | (_0xe7fex18 & ~_0xe7fex19), _0xe7fex12, _0xe7fex13, _0xe7fex15, _0xe7fex16, _0xe7fex17)
        }
          , _0xe7fex7 = function(_0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex15, _0xe7fex16, _0xe7fex17) {
            return _0xe7fex4(_0xe7fex13 ^ _0xe7fex18 ^ _0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex15, _0xe7fex16, _0xe7fex17)
        }
          , _0xe7fex8 = function(_0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex15, _0xe7fex16, _0xe7fex17) {
            return _0xe7fex4(_0xe7fex18 ^ (_0xe7fex13 | ~_0xe7fex19), _0xe7fex12, _0xe7fex13, _0xe7fex15, _0xe7fex16, _0xe7fex17)
        }
          , _0xe7fex9 = function(_0xe7fex15, _0xe7fex1a) {
            var _0xe7fex12 = _0xe7fex15[0]
              , _0xe7fex13 = _0xe7fex15[1]
              , _0xe7fex18 = _0xe7fex15[2]
              , _0xe7fex19 = _0xe7fex15[3];
            _0xe7fex12 = _0xe7fex5(_0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex1a[0], 7, -680876936);
            _0xe7fex19 = _0xe7fex5(_0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex1a[1], 12, -389564586);
            _0xe7fex18 = _0xe7fex5(_0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex1a[2], 17, 606105819);
            _0xe7fex13 = _0xe7fex5(_0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex1a[3], 22, -1044525330);
            _0xe7fex12 = _0xe7fex5(_0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex1a[4], 7, -176418897);
            _0xe7fex19 = _0xe7fex5(_0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex1a[5], 12, 1200080426);
            _0xe7fex18 = _0xe7fex5(_0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex1a[6], 17, -1473231341);
            _0xe7fex13 = _0xe7fex5(_0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex1a[7], 22, -45705983);
            _0xe7fex12 = _0xe7fex5(_0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex1a[8], 7, 1770035416);
            _0xe7fex19 = _0xe7fex5(_0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex1a[9], 12, -1958414417);
            _0xe7fex18 = _0xe7fex5(_0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex1a[10], 17, -42063);
            _0xe7fex13 = _0xe7fex5(_0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex1a[11], 22, -1990404162);
            _0xe7fex12 = _0xe7fex5(_0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex1a[12], 7, 1804603682);
            _0xe7fex19 = _0xe7fex5(_0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex1a[13], 12, -40341101);
            _0xe7fex18 = _0xe7fex5(_0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex1a[14], 17, -1502002290);
            _0xe7fex13 = _0xe7fex5(_0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex1a[15], 22, 1236535329);
            _0xe7fex12 = _0xe7fex6(_0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex1a[1], 5, -165796510);
            _0xe7fex19 = _0xe7fex6(_0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex1a[6], 9, -1069501632);
            _0xe7fex18 = _0xe7fex6(_0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex1a[11], 14, 643717713);
            _0xe7fex13 = _0xe7fex6(_0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex1a[0], 20, -373897302);
            _0xe7fex12 = _0xe7fex6(_0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex1a[5], 5, -701558691);
            _0xe7fex19 = _0xe7fex6(_0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex1a[10], 9, 38016083);
            _0xe7fex18 = _0xe7fex6(_0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex1a[15], 14, -660478335);
            _0xe7fex13 = _0xe7fex6(_0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex1a[4], 20, -405537848);
            _0xe7fex12 = _0xe7fex6(_0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex1a[9], 5, 568446438);
            _0xe7fex19 = _0xe7fex6(_0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex1a[14], 9, -1019803690);
            _0xe7fex18 = _0xe7fex6(_0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex1a[3], 14, -187363961);
            _0xe7fex13 = _0xe7fex6(_0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex1a[8], 20, 1163531501);
            _0xe7fex12 = _0xe7fex6(_0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex1a[13], 5, -1444681467);
            _0xe7fex19 = _0xe7fex6(_0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex1a[2], 9, -51403784);
            _0xe7fex18 = _0xe7fex6(_0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex1a[7], 14, 1735328473);
            _0xe7fex13 = _0xe7fex6(_0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex1a[12], 20, -1926607734);
            _0xe7fex12 = _0xe7fex7(_0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex1a[5], 4, -378558);
            _0xe7fex19 = _0xe7fex7(_0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex1a[8], 11, -2022574463);
            _0xe7fex18 = _0xe7fex7(_0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex1a[11], 16, 1839030562);
            _0xe7fex13 = _0xe7fex7(_0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex1a[14], 23, -35309556);
            _0xe7fex12 = _0xe7fex7(_0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex1a[1], 4, -1530992060);
            _0xe7fex19 = _0xe7fex7(_0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex1a[4], 11, 1272893353);
            _0xe7fex18 = _0xe7fex7(_0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex1a[7], 16, -155497632);
            _0xe7fex13 = _0xe7fex7(_0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex1a[10], 23, -1094730640);
            _0xe7fex12 = _0xe7fex7(_0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex1a[13], 4, 681279174);
            _0xe7fex19 = _0xe7fex7(_0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex1a[0], 11, -358537222);
            _0xe7fex18 = _0xe7fex7(_0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex1a[3], 16, -722521979);
            _0xe7fex13 = _0xe7fex7(_0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex1a[6], 23, 76029189);
            _0xe7fex12 = _0xe7fex7(_0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex1a[9], 4, -640364487);
            _0xe7fex19 = _0xe7fex7(_0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex1a[12], 11, -421815835);
            _0xe7fex18 = _0xe7fex7(_0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex1a[15], 16, 530742520);
            _0xe7fex13 = _0xe7fex7(_0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex1a[2], 23, -995338651);
            _0xe7fex12 = _0xe7fex8(_0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex1a[0], 6, -198630844);
            _0xe7fex19 = _0xe7fex8(_0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex1a[7], 10, 1126891415);
            _0xe7fex18 = _0xe7fex8(_0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex1a[14], 15, -1416354905);
            _0xe7fex13 = _0xe7fex8(_0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex1a[5], 21, -57434055);
            _0xe7fex12 = _0xe7fex8(_0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex1a[12], 6, 1700485571);
            _0xe7fex19 = _0xe7fex8(_0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex1a[3], 10, -1894986606);
            _0xe7fex18 = _0xe7fex8(_0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex1a[10], 15, -1051523);
            _0xe7fex13 = _0xe7fex8(_0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex1a[1], 21, -2054922799);
            _0xe7fex12 = _0xe7fex8(_0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex1a[8], 6, 1873313359);
            _0xe7fex19 = _0xe7fex8(_0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex1a[15], 10, -30611744);
            _0xe7fex18 = _0xe7fex8(_0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex1a[6], 15, -1560198380);
            _0xe7fex13 = _0xe7fex8(_0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex1a[13], 21, 1309151649);
            _0xe7fex12 = _0xe7fex8(_0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex1a[4], 6, -145523070);
            _0xe7fex19 = _0xe7fex8(_0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex18, _0xe7fex1a[11], 10, -1120210379);
            _0xe7fex18 = _0xe7fex8(_0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex13, _0xe7fex1a[2], 15, 718787259);
            _0xe7fex13 = _0xe7fex8(_0xe7fex13, _0xe7fex18, _0xe7fex19, _0xe7fex12, _0xe7fex1a[9], 21, -343485551);
            _0xe7fex15[0] = _0xe7fex3(_0xe7fex12, _0xe7fex15[0]);
            _0xe7fex15[1] = _0xe7fex3(_0xe7fex13, _0xe7fex15[1]);
            _0xe7fex15[2] = _0xe7fex3(_0xe7fex18, _0xe7fex15[2]);
            _0xe7fex15[3] = _0xe7fex3(_0xe7fex19, _0xe7fex15[3])
        }
          , _0xe7fexa = function(_0xe7fex16) {
            var _0xe7fex1b = [], _0xe7fex1c;
            for (_0xe7fex1c = 0; _0xe7fex1c < 64; _0xe7fex1c += 4) {
                _0xe7fex1b[_0xe7fex1c >> 2] = _0xe7fex16[__Ox2133f[3]](_0xe7fex1c) + (_0xe7fex16[__Ox2133f[3]](_0xe7fex1c + 1) << 8) + (_0xe7fex16[__Ox2133f[3]](_0xe7fex1c + 2) << 16) + (_0xe7fex16[__Ox2133f[3]](_0xe7fex1c + 3) << 24)
            }
            ;return _0xe7fex1b
        }
          , _0xe7fexb = function(_0xe7fex12) {
            var _0xe7fex1b = [], _0xe7fex1c;
            for (_0xe7fex1c = 0; _0xe7fex1c < 64; _0xe7fex1c += 4) {
                _0xe7fex1b[_0xe7fex1c >> 2] = _0xe7fex12[_0xe7fex1c] + (_0xe7fex12[_0xe7fex1c + 1] << 8) + (_0xe7fex12[_0xe7fex1c + 2] << 16) + (_0xe7fex12[_0xe7fex1c + 3] << 24)
            }
            ;return _0xe7fex1b
        }
          , _0xe7fexc = function(_0xe7fex16) {
            var _0xe7fex1d = _0xe7fex16[__Ox2133f[4]], _0xe7fex1e = [1732584193, -271733879, -1732584194, 271733878], _0xe7fex1c, _0xe7fex1f, _0xe7fex20, _0xe7fex21, _0xe7fex22, _0xe7fex23;
            for (_0xe7fex1c = 64; _0xe7fex1c <= _0xe7fex1d; _0xe7fex1c += 64) {
                _0xe7fex9(_0xe7fex1e, _0xe7fexa(_0xe7fex16[__Ox2133f[5]](_0xe7fex1c - 64, _0xe7fex1c)))
            }
            ;_0xe7fex16 = _0xe7fex16[__Ox2133f[5]](_0xe7fex1c - 64);
            _0xe7fex1f = _0xe7fex16[__Ox2133f[4]];
            _0xe7fex20 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0];
            for (_0xe7fex1c = 0; _0xe7fex1c < _0xe7fex1f; _0xe7fex1c += 1) {
                _0xe7fex20[_0xe7fex1c >> 2] |= _0xe7fex16[__Ox2133f[3]](_0xe7fex1c) << (_0xe7fex1c % 4 << 3)
            }
            ;_0xe7fex20[_0xe7fex1c >> 2] |= 0x80 << (_0xe7fex1c % 4 << 3);
            if (_0xe7fex1c > 55) {
                _0xe7fex9(_0xe7fex1e, _0xe7fex20);
                for (_0xe7fex1c = 0; _0xe7fex1c < 16; _0xe7fex1c += 1) {
                    _0xe7fex20[_0xe7fex1c] = 0
                }
            }
            ;_0xe7fex21 = _0xe7fex1d * 8;
            _0xe7fex21 = _0xe7fex21.toString(16)[__Ox2133f[6]](/(.*?)(.{0,8})$/);
            _0xe7fex22 = parseInt(_0xe7fex21[2], 16);
            _0xe7fex23 = parseInt(_0xe7fex21[1], 16) || 0;
            _0xe7fex20[14] = _0xe7fex22;
            _0xe7fex20[15] = _0xe7fex23;
            _0xe7fex9(_0xe7fex1e, _0xe7fex20);
            return _0xe7fex1e
        }
          , _0xe7fexd = function(_0xe7fex12) {
            var _0xe7fex1d = _0xe7fex12[__Ox2133f[4]], _0xe7fex1e = [1732584193, -271733879, -1732584194, 271733878], _0xe7fex1c, _0xe7fex1f, _0xe7fex20, _0xe7fex21, _0xe7fex22, _0xe7fex23;
            for (_0xe7fex1c = 64; _0xe7fex1c <= _0xe7fex1d; _0xe7fex1c += 64) {
                _0xe7fex9(_0xe7fex1e, _0xe7fexb(_0xe7fex12[__Ox2133f[7]](_0xe7fex1c - 64, _0xe7fex1c)))
            }
            ;_0xe7fex12 = _0xe7fex1c - 64 < _0xe7fex1d ? _0xe7fex12[__Ox2133f[7]](_0xe7fex1c - 64) : new Uint8Array(0);
            _0xe7fex1f = _0xe7fex12[__Ox2133f[4]];
            _0xe7fex20 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0];
            for (_0xe7fex1c = 0; _0xe7fex1c < _0xe7fex1f; _0xe7fex1c += 1) {
                _0xe7fex20[_0xe7fex1c >> 2] |= _0xe7fex12[_0xe7fex1c] << (_0xe7fex1c % 4 << 3)
            }
            ;_0xe7fex20[_0xe7fex1c >> 2] |= 0x80 << (_0xe7fex1c % 4 << 3);
            if (_0xe7fex1c > 55) {
                _0xe7fex9(_0xe7fex1e, _0xe7fex20);
                for (_0xe7fex1c = 0; _0xe7fex1c < 16; _0xe7fex1c += 1) {
                    _0xe7fex20[_0xe7fex1c] = 0
                }
            }
            ;_0xe7fex21 = _0xe7fex1d * 8;
            _0xe7fex21 = _0xe7fex21.toString(16)[__Ox2133f[6]](/(.*?)(.{0,8})$/);
            _0xe7fex22 = parseInt(_0xe7fex21[2], 16);
            _0xe7fex23 = parseInt(_0xe7fex21[1], 16) || 0;
            _0xe7fex20[14] = _0xe7fex22;
            _0xe7fex20[15] = _0xe7fex23;
            _0xe7fex9(_0xe7fex1e, _0xe7fex20);
            return _0xe7fex1e
        }
          , _0xe7fexe = [__Ox2133f[8], __Ox2133f[9], __Ox2133f[10], __Ox2133f[11], __Ox2133f[12], __Ox2133f[13], __Ox2133f[14], __Ox2133f[15], __Ox2133f[16], __Ox2133f[17], __Ox2133f[18], __Ox2133f[19], __Ox2133f[20], __Ox2133f[21], __Ox2133f[22], __Ox2133f[23]]
          , _0xe7fexf = function(_0xe7fex1d) {
            var _0xe7fex16 = __Ox2133f[24], _0xe7fex24;
            for (_0xe7fex24 = 0; _0xe7fex24 < 4; _0xe7fex24 += 1) {
                _0xe7fex16 += _0xe7fexe[(_0xe7fex1d >> (_0xe7fex24 * 8 + 4)) & 0x0f] + _0xe7fexe[(_0xe7fex1d >> (_0xe7fex24 * 8)) & 0x0f]
            }
            ;return _0xe7fex16
        }
          , _0xe7fex10 = function(_0xe7fex15) {
            var _0xe7fex1c;
            for (_0xe7fex1c = 0; _0xe7fex1c < _0xe7fex15[__Ox2133f[4]]; _0xe7fex1c += 1) {
                _0xe7fex15[_0xe7fex1c] = _0xe7fexf(_0xe7fex15[_0xe7fex1c])
            }
            ;return _0xe7fex15[__Ox2133f[25]](__Ox2133f[24])
        }
          , _0xe7fex11 = function(_0xe7fex16) {
            return _0xe7fex10(_0xe7fexc(_0xe7fex16))
        }
          , _0xe7fex2 = function() {
            this[__Ox2133f[26]]()
        };
        if (_0xe7fex11(__Ox2133f[27]) !== __Ox2133f[28]) {
            _0xe7fex3 = function(_0xe7fex15, _0xe7fex25) {
                var _0xe7fex26 = (_0xe7fex15 & 0xffff) + (_0xe7fex25 & 0xffff)
                  , _0xe7fex27 = (_0xe7fex15 >> 16) + (_0xe7fex25 >> 16) + (_0xe7fex26 >> 16);
                return (_0xe7fex27 << 16) | (_0xe7fex26 & 0xffff)
            }
        }
        ;_0xe7fex2[__Ox2133f[30]][__Ox2133f[29]] = function(_0xe7fex28) {
            if (/[\u0080-\uFFFF]/[__Ox2133f[31]](_0xe7fex28)) {
                _0xe7fex28 = unescape(encodeURIComponent(_0xe7fex28))
            }
            ;this[__Ox2133f[32]](_0xe7fex28);
            return this
        }
        ;
        _0xe7fex2[__Ox2133f[30]][__Ox2133f[32]] = function(_0xe7fex29) {
            this[__Ox2133f[33]] += _0xe7fex29;
            this[__Ox2133f[34]] += _0xe7fex29[__Ox2133f[4]];
            var _0xe7fex1f = this[__Ox2133f[33]][__Ox2133f[4]], _0xe7fex1c;
            for (_0xe7fex1c = 64; _0xe7fex1c <= _0xe7fex1f; _0xe7fex1c += 64) {
                _0xe7fex9(this._state, _0xe7fexa(this[__Ox2133f[33]][__Ox2133f[5]](_0xe7fex1c - 64, _0xe7fex1c)))
            }
            ;this[__Ox2133f[33]] = this[__Ox2133f[33]][__Ox2133f[35]](_0xe7fex1c - 64);
            return this
        }
        ;
        _0xe7fex2[__Ox2133f[30]][__Ox2133f[36]] = function(_0xe7fex2a) {
            var _0xe7fex2b = this[__Ox2133f[33]], _0xe7fex1f = _0xe7fex2b[__Ox2133f[4]], _0xe7fex1c, _0xe7fex20 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], _0xe7fex2c;
            for (_0xe7fex1c = 0; _0xe7fex1c < _0xe7fex1f; _0xe7fex1c += 1) {
                _0xe7fex20[_0xe7fex1c >> 2] |= _0xe7fex2b[__Ox2133f[3]](_0xe7fex1c) << (_0xe7fex1c % 4 << 3)
            }
            ;this._finish(_0xe7fex20, _0xe7fex1f);
            _0xe7fex2c = !!_0xe7fex2a ? this[__Ox2133f[37]] : _0xe7fex10(this._state);
            this[__Ox2133f[26]]();
            return _0xe7fex2c
        }
        ;
        _0xe7fex2[__Ox2133f[30]][__Ox2133f[38]] = function(_0xe7fex20, _0xe7fex1f) {
            var _0xe7fex1c = _0xe7fex1f, _0xe7fex21, _0xe7fex22, _0xe7fex23;
            _0xe7fex20[_0xe7fex1c >> 2] |= 0x80 << (_0xe7fex1c % 4 << 3);
            if (_0xe7fex1c > 55) {
                _0xe7fex9(this._state, _0xe7fex20);
                for (_0xe7fex1c = 0; _0xe7fex1c < 16; _0xe7fex1c += 1) {
                    _0xe7fex20[_0xe7fex1c] = 0
                }
            }
            ;_0xe7fex21 = this[__Ox2133f[34]] * 8;
            _0xe7fex21 = _0xe7fex21.toString(16)[__Ox2133f[6]](/(.*?)(.{0,8})$/);
            _0xe7fex22 = parseInt(_0xe7fex21[2], 16);
            _0xe7fex23 = parseInt(_0xe7fex21[1], 16) || 0;
            _0xe7fex20[14] = _0xe7fex22;
            _0xe7fex20[15] = _0xe7fex23;
            _0xe7fex9(this._state, _0xe7fex20)
        }
        ;
        _0xe7fex2[__Ox2133f[30]][__Ox2133f[26]] = function() {
            this[__Ox2133f[33]] = __Ox2133f[24];
            this[__Ox2133f[34]] = 0;
            this[__Ox2133f[37]] = [1732584193, -271733879, -1732584194, 271733878];
            return this
        }
        ;
        _0xe7fex2[__Ox2133f[30]][__Ox2133f[39]] = function() {
            delete this[__Ox2133f[37]];
            delete this[__Ox2133f[33]];
            delete this[__Ox2133f[34]]
        }
        ;
        _0xe7fex2[__Ox2133f[40]] = function(_0xe7fex28, _0xe7fex2a) {
            if (/[\u0080-\uFFFF]/[__Ox2133f[31]](_0xe7fex28)) {
                _0xe7fex28 = unescape(encodeURIComponent(_0xe7fex28))
            }
            ;var _0xe7fex2d = _0xe7fexc(_0xe7fex28);
            return !!_0xe7fex2a ? _0xe7fex2d : _0xe7fex10(_0xe7fex2d)
        }
        ;
        _0xe7fex2[__Ox2133f[41]] = function(_0xe7fex2e, _0xe7fex2a) {
            var _0xe7fex2d = _0xe7fexc(_0xe7fex2e);
            return !!_0xe7fex2a ? _0xe7fex2d : _0xe7fex10(_0xe7fex2d)
        }
        ;
        _0xe7fex2[__Ox2133f[42]] = function() {
            this[__Ox2133f[26]]()
        }
        ;
        _0xe7fex2[__Ox2133f[42]][__Ox2133f[30]][__Ox2133f[29]] = function(_0xe7fex2f) {
            var _0xe7fex2b = this._concatArrayBuffer(this._buff, _0xe7fex2f), _0xe7fex1f = _0xe7fex2b[__Ox2133f[4]], _0xe7fex1c;
            this[__Ox2133f[34]] += _0xe7fex2f[__Ox2133f[43]];
            for (_0xe7fex1c = 64; _0xe7fex1c <= _0xe7fex1f; _0xe7fex1c += 64) {
                _0xe7fex9(this._state, _0xe7fexb(_0xe7fex2b[__Ox2133f[7]](_0xe7fex1c - 64, _0xe7fex1c)))
            }
            ;this[__Ox2133f[33]] = _0xe7fex1c - 64 < _0xe7fex1f ? _0xe7fex2b[__Ox2133f[7]](_0xe7fex1c - 64) : new Uint8Array(0);
            return this
        }
        ;
        _0xe7fex2[__Ox2133f[42]][__Ox2133f[30]][__Ox2133f[36]] = function(_0xe7fex2a) {
            var _0xe7fex2b = this[__Ox2133f[33]], _0xe7fex1f = _0xe7fex2b[__Ox2133f[4]], _0xe7fex20 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], _0xe7fex1c, _0xe7fex2c;
            for (_0xe7fex1c = 0; _0xe7fex1c < _0xe7fex1f; _0xe7fex1c += 1) {
                _0xe7fex20[_0xe7fex1c >> 2] |= _0xe7fex2b[_0xe7fex1c] << (_0xe7fex1c % 4 << 3)
            }
            ;this._finish(_0xe7fex20, _0xe7fex1f);
            _0xe7fex2c = !!_0xe7fex2a ? this[__Ox2133f[37]] : _0xe7fex10(this._state);
            this[__Ox2133f[26]]();
            return _0xe7fex2c
        }
        ;
        _0xe7fex2[__Ox2133f[42]][__Ox2133f[30]][__Ox2133f[38]] = _0xe7fex2[__Ox2133f[30]][__Ox2133f[38]];
        _0xe7fex2[__Ox2133f[42]][__Ox2133f[30]][__Ox2133f[26]] = function() {
            this[__Ox2133f[33]] = new Uint8Array(0);
            this[__Ox2133f[34]] = 0;
            this[__Ox2133f[37]] = [1732584193, -271733879, -1732584194, 271733878];
            return this
        }
        ;
        _0xe7fex2[__Ox2133f[42]][__Ox2133f[30]][__Ox2133f[39]] = _0xe7fex2[__Ox2133f[30]][__Ox2133f[39]];
        _0xe7fex2[__Ox2133f[42]][__Ox2133f[30]][__Ox2133f[44]] = function(_0xe7fex30, _0xe7fex31) {
            var _0xe7fex32 = _0xe7fex30[__Ox2133f[4]]
              , _0xe7fex33 = new Uint8Array(_0xe7fex32 + _0xe7fex31[__Ox2133f[43]]);
            _0xe7fex33[__Ox2133f[45]](_0xe7fex30);
            _0xe7fex33[__Ox2133f[45]](new Uint8Array(_0xe7fex31), _0xe7fex32);
            return _0xe7fex33
        }
        ;
        _0xe7fex2[__Ox2133f[42]][__Ox2133f[40]] = function(_0xe7fex2f, _0xe7fex2a) {
            var _0xe7fex2d = _0xe7fexd(new Uint8Array(_0xe7fex2f));
            return !!_0xe7fex2a ? _0xe7fex2d : _0xe7fex10(_0xe7fex2d)
        }
        ;
        return _0xe7fex2
    }
    )());
    var _0xe7fex34 = __Ox2133f[46];
    function _0xe7fex35(_0xe7fex36) {
        function _0xe7fex37(_0xe7fex36) {
            var _0xe7fex38 = [];
            var _0xe7fex39 = {};
            for (var _0xe7fex3a in _0xe7fex36) {
                var _0xe7fex3b = {};
                _0xe7fex3b[__Ox2133f[47]] = _0xe7fex3a;
                var _0xe7fex3c = Object[__Ox2133f[30]][__Ox2133f[50]][__Ox2133f[49]](_0xe7fex36[_0xe7fex3a])[__Ox2133f[48]](8, -1);
                if (_0xe7fex3c === __Ox2133f[51]) {
                    _0xe7fex3b[__Ox2133f[52]] = _0xe7fex37(_0xe7fex36[_0xe7fex3a])
                } else {
                    if (_0xe7fex3c === __Ox2133f[53]) {
                        _0xe7fex3b[__Ox2133f[52]] = _0xe7fex36[_0xe7fex3a][__Ox2133f[56]](function(_0xe7fex3d) {
                            var _0xe7fex3e = Object[__Ox2133f[30]][__Ox2133f[50]][__Ox2133f[49]](_0xe7fex36[_0xe7fex3a])[__Ox2133f[48]](8, -1);
                            if (_0xe7fex3e === __Ox2133f[54] || _0xe7fex3e === __Ox2133f[55]) {
                                return __Ox2133f[24]
                            }
                            ;return String(_0xe7fex3d)
                        })
                    } else {
                        if (_0xe7fex3c === __Ox2133f[54] || _0xe7fex3c === __Ox2133f[55]) {
                            _0xe7fex3b[__Ox2133f[52]] = __Ox2133f[24]
                        } else {
                            _0xe7fex3b[__Ox2133f[52]] = String(_0xe7fex36[_0xe7fex3a])
                        }
                    }
                }
                ;_0xe7fex38[__Ox2133f[57]](_0xe7fex3b)
            }
            ;_0xe7fex38[__Ox2133f[58]](function(_0xe7fex12, _0xe7fex13) {
                return _0xe7fex12[__Ox2133f[47]] > _0xe7fex13[__Ox2133f[47]] ? 1 : _0xe7fex12[__Ox2133f[47]] < _0xe7fex13[__Ox2133f[47]] ? -1 : 0
            });
            _0xe7fex38[__Ox2133f[59]](function(_0xe7fex3f) {
                _0xe7fex39[_0xe7fex3f[__Ox2133f[47]]] = _0xe7fex3f[__Ox2133f[52]]
            });
            return _0xe7fex39
        }
        var _0xe7fex39 = _0xe7fex37(_0xe7fex36);
        return _0xe7fex2[__Ox2133f[40]](JSON[__Ox2133f[60]](_0xe7fex39) + _0xe7fex34)[__Ox2133f[48]](2, 12)
    }
    _0xe7fex1[__Ox2133f[69]](function(_0xe7fex40, _0xe7fex41) {
        var _0xe7fex42 = _0xe7fex1[__Ox2133f[62]](true, {}, _0xe7fex41[__Ox2133f[61]] || {});
        if (_0xe7fex42[__Ox2133f[63]]) {
            delete _0xe7fex42[__Ox2133f[63]]
        }
        ;_0xe7fex42[__Ox2133f[64]] = new Date()[__Ox2133f[65]]();
        var _0xe7fex43 = _0xe7fex35(_0xe7fex1[__Ox2133f[62]](true, {}, _0xe7fex42));
        if (__Ox2133f[61]in _0xe7fex40) {
            _0xe7fex40[__Ox2133f[61]] += (__Ox2133f[66] + _0xe7fex42[__Ox2133f[64]] + __Ox2133f[67] + _0xe7fex43)
        } else {
            _0xe7fex40[__Ox2133f[61]] = (__Ox2133f[68] + _0xe7fex42[__Ox2133f[64]] + __Ox2133f[67] + _0xe7fex43)
        }
    })
}
)()
```

### [mafengwo 爬虫](https://blog.csdn.net/wuyongfan6589/article/details/132776184)

获取淘宝商家信息
----------------

```python
# 获取商家信息
import pytest
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys
import time
import re
import csv

options = webdriver.ChromeOptions()
# options.add_argument("--headless")  # 启用无头模式
# options.add_argument("--disable-gpu")  # 禁用 GPU 加速
# options.add_argument("--window-size=1920,1080")  # 设置窗口大小
options.add_argument(r"--start-maximized")
options.add_experimental_option("detach", True)
options.add_argument(
    r"--user-data-dir=C:\Users\Administrator\AppData\Local\Google\Chrome\User Data"
)
options.add_argument(r"--profile-directory=Default")

driver = webdriver.Chrome(options=options)

driver.get(
    "https://s.taobao.com/search?commend=all&ie=utf8&initiative_id=tbindexz_20170306&localImgKey=&page=1&q=%E7%88%AC%E8%99%AB&search_type=item&sourceId=tb.index&spm=a21bo.jianhua%2Fa.201856.d13&ssid=s5-e&tab=all"
)


time.sleep(5)
bangs = []
shopers = driver.find_elements(
    by=By.XPATH,
    value='//*[@id="pageContent"]//div[@class="ShopInfo--shopInfo--ORFs6rK "]/div[1]/a',
)
for shoper in shopers:
    shoper_name = shoper.text
    shoper_url = shoper.get_attribute("href")
    print(shoper_name, shoper_url)
    bangs.append([shoper_name, shoper_url])

with open("shopper.csv", "w", encoding="utf-8") as f:
    csvfile = csv.writer(f)
    csvfile.writerow(["NAME", "URL"])
    csvfile.writerows(bangs)

with open("shopper.md", "w", encoding="utf-8") as f:
    f.write("SHOPPERR LIST\n")
    for name, url in bangs:
        f.write("(" + name + ")")
        f.write("[" + url + "]")
        f.write("\n")
```

```python
# 给商家发送请求
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys
import time
import re
import csv

options = webdriver.ChromeOptions()
# options.add_argument("--headless")  # 启用无头模式
# options.add_argument("--disable-gpu")  # 禁用 GPU 加速
# options.add_argument("--window-size=1920,1080")  # 设置窗口大小
options.add_argument(r"--start-maximized")
options.add_experimental_option("detach", True)
options.add_argument(
    r"--user-data-dir=C:\Users\Administrator\AppData\Local\Google\Chrome\User Data"
)
options.add_argument(r"--profile-directory=Default")

driver = webdriver.Chrome(options=options)
content = """您好！
我是一名充满热情的网页工程师，擅长使用前端技术（如HTML、CSS、JavaScript）和后端技术（如Node.js、Python）进行全栈开发。我的技术栈还包括现代框架如Vue.js，以及数据库管理（如MySQL、MongoDB）。我注重用户体验，致力于编写高质量、可维护的代码，并且在团队合作中表现出色。
我近期完成了一个爬虫项目，展示了我在数据抓取和分析方面的能力。该爬虫利用Python和Selenium等工具，自动化收集和整理各大招聘网站的职位信息，为求职者提供精准的职位推荐。项目的成功展示了我在编写自动化脚本、处理复杂网页结构以及数据处理方面的熟练度。
我希望能有机会加入贵公司，与贵团队共同成长，为公司贡献自己的力量。期待您的回复！
"""
with open("./shopper.csv", "r", encoding="utf-8") as f:
    csvreader = csv.reader(f)
    head = next(csvreader)
    for row in csvreader:
        if row:
            print(row)
            name = row[0]
            url = row[1]
            driver.get(url=url)
            try:
                driver.find_element(
                    by=By.XPATH,
                    value="//a[@target=\"_blank\"][contains(text(), '联系客服')]",
                ).click()
                time.sleep(3)
                driver.switch_to.window(driver.window_handles[1])
                # 等待iframe加载完成
                WebDriverWait(driver, 10).until(
                    EC.frame_to_be_available_and_switch_to_it((By.XPATH, "//iframe[1]"))
                )

                # 等待发送按钮加载完成
                send_button = WebDriverWait(driver, 10).until(
                    EC.element_to_be_clickable((By.XPATH, "//button"))
                )
                editable_pre = driver.find_element(
                    By.CSS_SELECTOR, 'pre.edit[contenteditable="true"]'
                )
                time.sleep(2)
                editable_pre.send_keys(
                    r"你好,我擅长爬虫和网页开发,请问你们招人吗,我想要加入.我的手机号是15247837258,微信同号,欢迎打扰"
                )
                # 执行发送操作（假设有发送按钮）
                send_button = driver.find_element(By.XPATH, "//button")
                send_button.click()
                print("now is ", name)
                driver.switch_to.window(driver.window_handles[0])
            except:
                driver.switch_to.window(driver.window_handles[0])
                continue
```

淘宝抢单爬虫
------------

[使用selenium模拟实现](https://github.com/wangkun100/-/blob/main/python%E6%8A%A2%E5%8D%95%E6%BA%90%E7%A0%81)

验证码
------

[冰拓打码平台](https://www.bingtop.com/demo/)

参考mhtml[里面的验证码分类](https://www.bingtop.com/type/#type_coordinate)



爬虫问题汇总
============

晋江文学网
----------

用户名 414497103@qq.com

密码 h19880707*

[从百度地图上获取充电桩数据](https://blog.csdn.net/DAIBISON/article/details/104831312)
--------------------------------------------------------------------------------------

百度地图网页端只能获取充电桩的位置信息，不能获取充电桩的状态

[美团爬取商家数据](https://www.meituan.com/)
--------------------------------------------

美团网页端没有数据，只能使用app

[豆瓣](https://book.douban.com/tag/)
------------------------------------

豆瓣是一个免登录就能获取信息的网站，是爬虫开始练手的绝佳网站

[博客园](https://www.cnblogs.com/)
----------------------------------

博客园是一个技术类型的网站，他也比较适合爬虫练手

[考试宝](https://www.kaoshibao.com/docs/)
-----------------------------------------

从上面获取题目，然后组合重新出题

[医院](https://qlck.linkedcare.cn/LogOn?ReturnUrl=%2F#/dashboard)
-----------------------------------------------------------------

根据门诊号获得影像图片，然后筛选出知情同意书，并且识别出里面的内容

用户名：质控测试

密码：zkcs789

[eva](https://www3.advantest.com/products/electronic-measuring-instruments/eva100)
----------------------------------------------------------------------------------

这是个已经有70年的网站，最近要下架了